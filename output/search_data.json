{
  
    "docs-fluo-1-2-administration-initialize": {
      "title": "Initialize Application",
      "content"	 : "OverviewBefore a Fluo Application can run, it must be initialized.  Below is an overview of whatinitialization does and some of the properties that may be set for initialization.  Initialize ZooKeeper - Each application has its own area in ZooKeeper used for configuration,Oracle state, and worker coordination. All properties, except fluo.connections.*, are copiedinto ZooKeeper. For example, if fluo.worker.num.threads=128 was set, then when a worker processstarts it will read this from ZooKeeper.  Copy Observer jars to DFS - Fluo workers processes need the jars containing observers. Theseare provided in one of the following ways.          Set the property fluo.observer.init.dir to a local directory containing observer jars. Thejars in this directory are copied to DFS under &amp;lt;fluo.dfs.root&amp;gt;/&amp;lt;app name&amp;gt;. When a worker isstarted, the jars are pulled from DFS and added to its classpath.      Set the property fluo.observer.jars.url to a directory in DFS containing observer jars.  Nocopying is done. When a worker is started, the jars are pulled from this location and added toits classpath.      Do not set any of the properties above and have the mechanism that starts the worker processadd the needed jars to the classpath.        Create Accumulo table - Each Fluo application creates and configures an Accumulo table. Thefluo.accumulo.* properties determine which Accumulo instance is used. For performance reasons,Fluo runs its own code in Accumulo tablet servers. Fluo attempts to copy Fluo jars into DFS andconfigure Accumulo to use them. Fluo first checks the property fluo.accumulo.jars and if set,copies the jars listed there. If that property is not set, then Fluo looks on the classpath tofind jars. Jars are copied to a location under &amp;lt;fluo.dfs.root&amp;gt;/&amp;lt;app name&amp;gt;.InstructionsBelow are the steps to initialize an application from the command line. It is also possible toinitialize an application using Fluo’s Java API.      Create a copy of fluo-app.properties for your Fluo application.     cp $FLUO_HOME/conf/fluo-app.properties /path/to/myapp/fluo-app.properties            Edit your copy of fluo-app.properties and make sure to set the following:          Class name of your ObserverProvider      Paths to your Fluo observer jars      Accumulo configuration      DFS configuration        When configuring the observer section of fluo-app.properties, you can configure your instance for thephrasecount application if you have not created your own application. See the phrasecountexample for instructions. You can also choose not to configure any observers but your workers willbe idle when started.        Run the command below to initialize your Fluo application. Change myapp to your application name:     fluo init -a myapp -p /path/to/myapp/fluo-app.properties        A Fluo application only needs to be initialized once. After initialization, the Fluo applicationname is used to start/stop the application and scan the Fluo table.        Run fluo list which connects to Fluo and lists applications to verify initialization.        Run fluo config -a myapp to see what configuration is stored in ZooKeeper.  Next StepsRun your Fluo application using one of the methods below:  Run Fluo processes  Run Fluo in YARN  Run Fluo in Docker which enables running in Mesos and KubernetesAlso, learn how to manage Fluo applications.",
      "url": " /docs/fluo/1.2/administration/initialize",
      "categories": "administration"
    },
  
    "docs-fluo-1-2-administration-manage-applications": {
      "title": "Manage Applications",
      "content"	 : "This page contains information on managing Fluo applications.Viewing dataWhen you have data in your Fluo application, you can view it using the command fluo scan -a myapp. Pipe the output to less using the command fluo scan -a myapp | less if you want to page through the data.Get status of all applicationsTo list all Fluo applications, run fluo list.Running application codeThe fluo exec &amp;lt;app name&amp;gt; &amp;lt;class&amp;gt; {arguments} provides an easy way to execute application code. Itwill execute a class with a main method if a jar containing the class is included with the observer jars configured at initialization. When the class is run, Fluo classes and dependencies will be on the classpath. The fluo exec command can inject the applications configuration if the class is written in the following way. Defining the injection point is optional.import javax.inject.Inject;public class AppCommand {  //when run with fluo exec command, the applications configuration will be injected  @Inject  private static FluoConfiguration fluoConfig;  public static void main(String[] args) throws Exception {    try(FluoClient fluoClient = FluoFactory.newClient(fluoConfig)) {      //do stuff with Fluo    }  }}Application ConfigurationFor configuring observers, fluo provides a simple mechanism to set and access application specificconfiguration.  See the javadoc on FluoClient.getAppConfiguration() for more details.Debugging ApplicationsWhile monitoring Fluo metrics can detect problems (like too many transaction collisions)in a Fluo application, metrics may not provide enough information to debug the root causeof the problem. To help debug Fluo applications, low-level logging of transactions can be turned onby setting the following loggers to TRACE:            Logger      Level      Information                  fluo.tx      TRACE      Provides detailed information about what transactions read and wrote              fluo.tx.summary      TRACE      Provides a one line summary about each transaction executed              fluo.tx.collisions      TRACE      Provides details about what data was involved When a transaction collides with another transaction              fluo.tx.scan      TRACE      Provides logging for each cell read by a scan.  Scan summary logged at fluo.tx level.  This allows suppression of fluo.tx.scan while still seeing summary.      Below is an example log after setting fluo.tx to TRACE. The number following txid:  is thetransactions start timestamp from the Oracle.2015-02-11 18:24:05,341 [fluo.tx ] TRACE: txid: 3 begin() thread: 1982015-02-11 18:24:05,343 [fluo.tx ] TRACE: txid: 3 class: com.SimpleLoader2015-02-11 18:24:05,357 [fluo.tx ] TRACE: txid: 3 get(4333, stat count ) -&amp;gt; null2015-02-11 18:24:05,357 [fluo.tx ] TRACE: txid: 3 set(4333, stat count , 1)2015-02-11 18:24:05,441 [fluo.tx ] TRACE: txid: 3 commit() -&amp;gt; SUCCESSFUL commitTs: 42015-02-11 18:24:05,341 [fluo.tx ] TRACE: txid: 5 begin() thread: 1982015-02-11 18:24:05,442 [fluo.tx ] TRACE: txid: 3 close()2015-02-11 18:24:05,343 [fluo.tx ] TRACE: txid: 5 class: com.SimpleLoader2015-02-11 18:24:05,357 [fluo.tx ] TRACE: txid: 5 get(4333, stat count ) -&amp;gt; 12015-02-11 18:24:05,357 [fluo.tx ] TRACE: txid: 5 set(4333, stat count , 2)2015-02-11 18:24:05,441 [fluo.tx ] TRACE: txid: 5 commit() -&amp;gt; SUCCESSFUL commitTs: 62015-02-11 18:24:05,442 [fluo.tx ] TRACE: txid: 5 close()The log above traces the following sequence of events.  Transaction T1 has a start timestamp of 3  Thread with id 198 is executing T1, its running code from the class com.SimpleLoader  T1 reads row 4333 and column stat count which does not exist  T1 sets row 4333 and column stat count to 1  T1 commits successfully and its commit timestamp from the Oracle is 4.  Transaction T2 has a start timestamp of 5 (because its 5 &amp;gt; 4 it can see what T1 wrote).  T2 reads a value of 1 for row 4333 and column stat count  T2 sets row 4333 and column stat count to 2  T2 commits successfully with a commit timestamp of 6Below is an example log after only setting fluo.tx.collisions to TRACE. This setting will only logtrace information when a collision occurs. Unlike the previous example, what the transaction readand wrote is not logged. This shows that a transaction with a start timestamp of 106 and a classname of com.SimpleLoader collided with another transaction on row r1 and column fam1 qual1.2015-02-11 18:17:02,639 [tx.collisions] TRACE: txid: 106 class: com.SimpleLoader2015-02-11 18:17:02,639 [tx.collisions] TRACE: txid: 106 collisions: {r1=[fam1 qual1 ]}When applications read and write arbitrary binary data, this does not log so well. In order to makethe trace logs human readable, non ASCII chars are escaped using hex. The convention used it xDDwhere D is a hex digit. Also the  character is escaped to make the output unambiguous.",
      "url": " /docs/fluo/1.2/administration/manage-applications",
      "categories": "administration"
    },
  
    "docs-fluo-1-2-administration-metrics": {
      "title": "Metrics",
      "content"	 : "A Fluo application can be configured (in fluo-app.properties) to report metrics. When metrics areconfigured, Fluo will report some ‘default’ metrics about an application that help users monitor itsperformance. Users can also write code to report ‘application-specific’ metrics from theirapplications. Both ‘application-specific’ and ‘default’ metrics share the same reporter configuredby fluo-app.properties and are described in detail below.Configuring reportersFluo metrics are not published by default. To publish metrics, configure a reporter in the ‘metrics’section of fluo-app.properties. There are several different reporter types (i.e Console, CSV,Graphite, JMX, SLF4J) that are implemented using Dropwizard. The choice of which reporter to usedepends on the visualization tool used. If you are not currently using a visualization tool, thereis documentation at end of this page for reporting Fluo metrics to Grafana/InfluxDB.Metrics namesWhen Fluo metrics are reported, they are published using a naming scheme that encodes additionalinformation. This additional information is represented using all caps variables (i.e METRIC)below.Default metrics start with fluo.class or fluo.system and have following naming schemes:    fluo.class.APPLICATION.REPORTER_ID.METRIC.CLASS    fluo.system.APPLICATION.REPORTER_ID.METRICApplication metrics start with fluo.app and have following scheme:    fluo.app.REPORTER_ID.METRICThe variables below describe the additional information that is encoded in metrics names.  APPLICATION - Fluo application name  REPORTER_ID - Unique ID of the Fluo oracle, worker, or client that is reporting the metric. When running in YARN, this ID is of the format worker-INSTANCE_ID or oracle-INSTANCE_ID where INSTANCE_ID corresponds to instance number. When not running in YARN, this ID consists of a hostname and a base36 long that is unique across all fluo processes.  METRIC - Name of the metric. For ‘default’ metrics, this is set by Fluo. For ‘application’ metrics, this is set by user. Name should be unique and avoid using period ‘.’ in name.  CLASS - Name of Fluo observer or loader class that produced metric. This allows things like transaction collisions to be tracked per class.Application-specific metricsApplication metrics are implemented by retrieving a MetricsReporter from an Observer, Loader,or FluoClient.  These metrics are named using the format fluo.app.REPORTER_ID.METRIC.Default metricsDefault metrics report for a particular Observer/Loader class or system-wide.Below are metrics that are reported from each Observer/Loader class that is configured in a Fluoapplication. These metrics are reported after each transaction and named using the formatfluo.class.APPLICATION.REPORTER_ID.METRIC.CLASS.  tx_lock_wait_time - Timer          Time transaction spent waiting on locks held by other transactions.      Only updated for transactions that have non-zero lock time.        tx_execution_time - Timer          Time transaction took to execute.      Updated for failed and successful transactions.      This does not include commit time, only the time from start until commit is called.        tx_with_collision - Meter          Rate of transactions with collisions.        tx_collisions - Meter          Rate of collisions.        tx_entries_set - Meter          Rate of row/columns set by transaction        tx_entries_read - Meter          Rate of row/columns read by transaction that existed.      There is currently no count of all reads (including non-existent data)        tx_locks_timedout - Meter          Rate of timedout locks rolled back by transaction.      These are locks that are held for very long periods by another transaction that appears to bealive based on zookeeper.        tx_locks_dead - Meter          Rate of dead locks rolled by a transaction.      These are locks held by a process that appears to be dead according to zookeeper.        tx_status_&amp;lt;STATUS&amp;gt; - Meter          Rate of different ways (i.e &amp;lt;STATUS&amp;gt;) a transaction can terminate      Below are system-wide metrics that are reported for the entire Fluo application. These metrics arenamed using the format fluo.system.APPLICATION.REPORTER_ID.METRIC.  oracle_response_time - Timer          Time each RPC call to oracle for stamps took        oracle_client_stamps - Histogram          Number of stamps requested for each request for stamps to the server        oracle_server_stamps - Histogram          Number of stamps requested for each request for stamps from a client        worker_notifications_queued - Gauge          The current number of notifications queued for processing.        transactor_committing - Gauge          The current number of transactions that are working their way through the commit steps.      Histograms and Timers have a counter. In the case of a histogram, the counter is the number of timesthe metric was updated and not a sum of the updates. For example if a request for 5 timestamps wasmade to the oracle followed by a request for 3 timestamps, then the count for oracle_server_stampswould be 2 and the mean would be (5+3)/2.View metrics in Grafana/InfuxDBFluo metrics can be sent to InfluxDB, a time series database, and made viewable in Grafana, a visualization tool. Below is a screenshot of Fluo metrics in Grafana:Following the instructions below to set this up:      Follow the standard installation instructions for InfluxDB and Grafana. As for versions,the instructions below were written using InfluxDB v0.9.4.2 and Grafana v2.5.0.        Add the following to your InfluxDB configuration to configure it accept metrics in Graphiteformat from Fluo. The configuration below contains templates that transform the Graphitemetrics into a format that is usable in InfluxDB.    [[graphite]]  bind-address = &quot;:2003&quot;  enabled = true  database = &quot;fluo_metrics&quot;  protocol = &quot;tcp&quot;  consistency-level = &quot;one&quot;  separator = &quot;_&quot;  batch-size = 1000  batch-pending = 5  batch-timeout = &quot;1s&quot;  templates = [    &quot;fluo.class.*.*.*.*.* ..app.host.measurement.observer.field&quot;,    &quot;fluo.class.*.*.*.* ..app.host.measurement.observer&quot;,    &quot;fluo.system.*.*.*.* ..app.host.measurement.field&quot;,    &quot;fluo.system.*.*.* ..app.host.measurement&quot;,    &quot;fluo.app.*.*.* ..host.measurement.field&quot;,    &quot;fluo.app.*.* ..host.measurement&quot;,  ]            Fluo distributes a file called fluo_metrics_setup.txt that contains a list of commands thatsetup InfluxDB. These commands will configure an InfluxDB user, retention policies, andcontinuous queries that downsample data for the historical dashboard in Grafana. Run the commandbelow to execute the commands in this file:     $INFLUXDB_HOME/bin/influx -import -path $FLUO_HOME/contrib/influxdb/fluo_metrics_setup.txt            Configure the fluo-app.properties of your Fluo application to send Graphite metrics to InfluxDB.Below is example configuration. Remember to replace &amp;lt;INFLUXDB_HOST&amp;gt; with the actual host.     fluo.metrics.reporter.graphite.enable=true fluo.metrics.reporter.graphite.host=&amp;lt;INFLUXDB_HOST&amp;gt; fluo.metrics.reporter.graphite.port=2003 fluo.metrics.reporter.graphite.frequency=30        The reporting frequency of 30 sec is required if you are using the provided Grafana dashboards that are configured in the next step.        Grafana needs to be configured to load dashboard JSON templates from a directory. Fluodistributes two Grafana dashboard templates in its tarball distribution in the directorycontrib/grafana. Before restarting Grafana, you should copy the templates from your Fluoinstallation to the dashboards/ directory configured below.    [dashboards.json]enabled = truepath = &amp;lt;GRAFANA_HOME&amp;gt;/dashboards            If you restart Grafana, you will see the Fluo dashboards configured but all of their charts willbe empty unless you have a Fluo application running and configured to send data to InfluxDB.When you start sending data, you may need to refresh the dashboard page in the browser to startviewing metrics.  ",
      "url": " /docs/fluo/1.2/administration/metrics",
      "categories": "administration"
    },
  
    "docs-fluo-1-2-administration-run-fluo-in-docker": {
      "title": "Run Fluo in Docker",
      "content"	 : "Obtain the Docker imageTo obtain the docker image created by this project, you can either pull it from DockerHub atapache/fluo or build it yourself. To pull the image from DockerHub, run the command below:docker pull apache/fluoWhile it is easier to pull from DockerHub, the image will default to the software versions below:            Software      Version                  Fluo      1.2.0              Accumulo      1.8.1              Hadoop      2.7.5              Zookeeper      3.4.11      If these versions do not match what is running on your cluster, you should consider buildingyour own image with matching versions.Image basicsThe entrypoint for the Fluo docker image is the fluo script. While the primary usecase for this image is to start an oracle or worker, you can run other commands in thefluo script to test out the image:# No arguments prints Fluo command usagedocker run apache/fluo# Print Fluo versiondocker run apache/fluo version# Print Fluo classpathdocker run apache/fluo classpathThis Docker image only runs Fluo, it does not run Accumulo, Hadoop, orZooKeeper processes.  These systems must be run separately and the Docker imageconfigured to use them.Initialize applicationBefore starting a Fluo oracle and worker using the Fluo Docker image,initialize your Fluo application.  Initialization can also bedone using Docker.  To do this, the applications jars and config file need tobe mapped into the docker container.  To understand how to do this, assume thefollowing setup for local files outside of Docker.  Application jars in /home/user1/myapp/lib  fluo-app.properties in /home/user1/myapp/conf  fluo.observer.init.dir=/opt/myapp/lib is set in fluo-app.properties along with any other properties neededWith this setup, the following Docker command will initialize myapp. Fluoinit runs in docker as root, so HADOOP_USER_NAME is set in order to avoidcopying into HDFS as root.  The Docker run option -v/home/user1/myapp:/opt/myapp maps the local directory /home/user1/myapp intothe container at /opt/myapp.docker run --network=&quot;host&quot; -v /home/user1/myapp:/opt/myapp            -e HADOOP_USER_NAME=`whoami`            apache/fluo init -a myapp -f -p /opt/myapp/conf/fluo-app.properties                             -o fluo.connection.zookeepers=zkhost/fluoRun application in DockerAfter initializing, choose a method below to run the oracle and worker(s) of your Fluo application. In the examples below, the Fluoapplication is named myapp and was initialized using a Zookeeper node on zkhost.Docker engineUse the docker command to start local docker containers.      Start a Fluo oracle for myapp application connecting to a Zookeeper at zkhost.     docker run -d --network=&quot;host&quot; apache/fluo oracle -a myapp -o fluo.connection.zookeepers=zkhost/fluo        The command above uses -d to run the container in the background and --network=&quot;host&quot; to run the container on the host’s network stack.        Start Fluo worker(s). Execute this command multiple times to start multiple workers.     docker run -d --network=&quot;host&quot; apache/fluo worker -a myapp -o fluo.connection.zookeepers=zkhost/fluo      MarathonUsing the Marathon UI, you can create applications using JSON configuration.The JSON below can be used to start a Fluo oracle.{  &quot;id&quot;: &quot;myapp-fluo-oracle&quot;,  &quot;cmd&quot;: &quot;fluo oracle -a myapp -o fluo.connection.zookeepers=zkhost/fluo&quot;,  &quot;cpus&quot;: 1,  &quot;mem&quot;: 256,  &quot;disk&quot;: 0,  &quot;instances&quot;: 1,  &quot;container&quot;: {    &quot;docker&quot;: {      &quot;image&quot;: &quot;apache/fluo&quot;,      &quot;network&quot;: &quot;HOST&quot;    },    &quot;type&quot;: &quot;DOCKER&quot;  }}The JSON below can be used to start Fluo worker(s). Modify instances to start multiple workers.{  &quot;id&quot;: &quot;myapp-fluo-worker&quot;,  &quot;cmd&quot;: &quot;fluo worker -a myapp -o fluo.connection.zookeepers=zkhost/fluo&quot;,  &quot;cpus&quot;: 1,  &quot;mem&quot;: 512,  &quot;disk&quot;: 0,  &quot;instances&quot;: 1,  &quot;container&quot;: {    &quot;docker&quot;: {      &quot;image&quot;: &quot;apache/fluo&quot;,      &quot;network&quot;: &quot;HOST&quot;    },    &quot;type&quot;: &quot;DOCKER&quot;  }}Next StepsLearn how to manage Fluo applications.",
      "url": " /docs/fluo/1.2/administration/run-fluo-in-docker",
      "categories": "administration"
    },
  
    "docs-fluo-1-2-administration-run-fluo-in-yarn": {
      "title": "Run Fluo in YARN",
      "content"	 : "An Apache Fluo application can be started in Hadoop YARN using the Fluo YARN launcher.RequirementsTo launch a Fluo application in YARN, you’ll need the following software installed.            Software      Recommended Version      Minimum Version                  Fluo      1.2.0      1.2.0              Fluo YARN      1.0.0      1.0.0              YARN      2.7.2      2.6.0      Instructions for installing Fluo YARN can be found below. See the related projects page for external projectsthat may help in setting up all of these dependencies.Set up your Fluo applicationBefore you can launch a Fluo application in YARN, you should install Fluo and initialize yourapplication. After your application has been initialized, follow the instructions below to installthe Fluo YARN launcher and run your application in YARN. Avoid using the fluo command to start local oracleand worker processes if you are running in YARN.Install and Configure Fluo YARN launcherTo install the Fluo YARN launcher, you will need to obtain a distribution tarball. It is recommended that youdownload the latest release. You can also build a distribution from the main branch by following these stepswhich create a tarball in distribution/target:git clone https://github.com/apache/fluo-yarn.gitcd fluo-yarn/mvn packageAfter you obtain a Fluo YARN distribution tarball, follow these steps to install Fluo.      Choose a directory with plenty of space, untar the distribution, and run fetch.sh to retrieve dependencies:     tar -xvzf fluo-yarn-1.0.0-bin.tar.gz cd fluo-yarn-1.0.0 ./lib/fetch.sh        The distribution contains a fluo-yarn script in bin/ that administers Fluo and the following configuration files in conf/:                            Configuration file          Description                                      fluo-yarn-env.sh          Configures classpath for fluo-yarn script. Required for all commands.                          fluo-yarn.properties          Configures how application runs in YARN.  Required for start command.                          log4j.properties          Configures logging                          Configure fluo-yarn-env.sh          Set FLUO_HOME if it is not in your environment      Modify FLUO_CONN_PROPS if you don’t want use the default.            Configure fluo-yarn.properties to set how your application will be launched in YARN:          YARN resource manager hostname      Number of oracle and worker instances      Max memory usage per oracle/worker        If you are managing multiple Fluo applications in YARN, you can copy this file and configure it foreach application.  Start Fluo application in YARNFollow the instructions below to start your application in YARN. If you have not done so already, you should initializeyour Fluo application before following these instructions.      Configure fluo-yarn-env.sh and fluo-yarn.properties if you have not already.        Run the commands below to start your Fluo application in YARN.     fluo-yarn start myapp conf/fluo-yarn.properties        The commands will retrieve your application configuration and observer jars (using your application name) beforestarting the application in YARN. The command will output a YARN application ID that can be used to find yourapplication in the YARN resource manager and view its logs.  Manage Fluo application in YARNExcept for stopping your application in YARN, the fluo script can be used to manage your application using thescan and wait commands.When you want you stop your Fluo application, use the the YARN resource manager or the yarn application -kill &amp;lt;App ID&amp;gt; to stop the application in YARN.",
      "url": " /docs/fluo/1.2/administration/run-fluo-in-yarn",
      "categories": "administration"
    },
  
    "docs-fluo-1-2-administration-run-fluo-processes": {
      "title": "Run Fluo processes",
      "content"	 : "An Apache Fluo application consists of an oracle process and multiple worker processes.  These processescan be manually started by users on a single node or distributed across a cluster.RequirementsEach node where Fluo is run must have Fluo installed.Start Fluo processesFollow the instructions below to start Fluo processes.      Configure fluo-env.sh and fluo-conn.properties if you have not already.        Run Fluo application processes using the fluo oracle and fluo worker commands. Fluo applicationsare typically run with one oracle process and multiple worker processes. The commands below will starta Fluo oracle and two workers on your local machine:     fluo oracle -a myapp &amp;amp;&amp;gt; oracle.log &amp;amp; fluo worker -a myapp &amp;amp;&amp;gt; worker1.log &amp;amp; fluo worker -a myapp &amp;amp;&amp;gt; worker2.log &amp;amp;        The commands will retrieve your application configuration and observer jars (using yourapplication name) before starting the oracle or worker process.  If you want to distribute the processes of your Fluo application across a cluster, you will need installFluo on every node where you want to run a Fluo process. It is recommended that you use a cluster managementtool like Salt, Ansible, or Chef to install and run Fluo on a cluster.Stop Fluo processesTo stop your Fluo application, run jps -m | grep Fluo to find process IDs and use kill to stop them.Next StepsLearn how to manage Fluo applications.",
      "url": " /docs/fluo/1.2/administration/run-fluo-processes",
      "categories": "administration"
    },
  
    "docs-fluo-1-2-administration-troubleshooting": {
      "title": "Troubleshooting",
      "content"	 : "Steps for troubleshooting problems with Fluo applications.Fluo application stops processing data  Confirm that your application is running with the expected number of workers.     $ fluo list Fluo instance (localhost/fluo) contains 1 application(s) Application     Status     # Workers -----------     ------     --------- webindex        RUNNING        3        Look for errors in the logs of any oracle or worker that has died.    Run the fluo wait command to see if you application is processing notifications.     $ fluo wait -a webindex [command.FluoWait] INFO : The wait command will exit when all notifications are processed [command.FluoWait] INFO : 140 notifications are still outstanding.  Will try again in 10 seconds... [command.FluoWait] INFO : 140 notifications are still outstanding.  Will try again in 10 seconds... [command.FluoWait] INFO : 140 notifications are still outstanding.  Will try again in 10 seconds... [command.FluoWait] INFO : 96 notifications are still outstanding.  Will try again in 10 seconds... [command.FluoWait] INFO : 70 notifications are still outstanding.  Will try again in 10 seconds... [command.FluoWait] INFO : 31 notifications are still outstanding.  Will try again in 10 seconds... [command.FluoWait] INFO : All processing has finished!        The number of notifications will increase as data is added to the application but they should eventually decreaseto zero and processing should finish.        Look for errors or exceptions in the logs of all oracle and worker processes. Processing can stop if all threadsin a worker process were consumed by exceptions thrown in Fluo application’s observer code. These exceptionsare often due to parsing issues or corner cases not seen during development or using small data sets.        If you are using a cluster manager (i.e Marathon, YARN etc) to run your Fluo application, look for errors in the logs ofyour cluster manager or application manager.  Below are some common errors:          Cluster managers sometimes fail to start all process of Fluo application due to lack of container slots or resources (CPU, memory, etc).This can be fixed by giving more resources to your cluster manager or decrease the number/resources of Fluo workers.      Cluster managers can kill Fluo processes if they use too much memory. This can be fixed by allocating more memory to your workers.            Run jstack to get stack traces of threads in your Fluo application processes and look for any stuck threads.    Consider configuring your Fluo application to report metrics so that they are viewable in Grafana/InfluxDB. Metricscan are helpfu in debugging performance issues.If you are still having trouble, feel free to email dev@fluo.apache.org for help.",
      "url": " /docs/fluo/1.2/administration/troubleshooting",
      "categories": "administration"
    },
  
    "docs-fluo-1-2-getting-started-create-application": {
      "title": "Create Application",
      "content"	 : "Once you have Fluo installed, you can create and run Fluo applications consistingof clients and observers. This documentation shows how to create a Fluo client and observer.Fluo Maven DependenciesFor both clients and observers, you will need to include the following in your Maven pom:&amp;lt;dependency&amp;gt;  &amp;lt;groupId&amp;gt;org.apache.fluo&amp;lt;/groupId&amp;gt;  &amp;lt;artifactId&amp;gt;fluo-api&amp;lt;/artifactId&amp;gt;  &amp;lt;version&amp;gt;1.2.0&amp;lt;/version&amp;gt;&amp;lt;/dependency&amp;gt;&amp;lt;dependency&amp;gt;  &amp;lt;groupId&amp;gt;org.apache.fluo&amp;lt;/groupId&amp;gt;  &amp;lt;artifactId&amp;gt;fluo-core&amp;lt;/artifactId&amp;gt;  &amp;lt;version&amp;gt;1.2.0&amp;lt;/version&amp;gt;  &amp;lt;scope&amp;gt;runtime&amp;lt;/scope&amp;gt;&amp;lt;/dependency&amp;gt;Fluo provides a classpath command to help users build a runtime classpath. This command along withthe hadoop jar command is useful when writing scripts to run Fluo client code. These commandsallow the scripts to use the versions of Hadoop, Accumulo, and Zookeeper installed on a cluster.Creating a Fluo clientTo create a FluoClient, you will need to provide it with a FluoConfiguration object that isconfigured to connect to your Fluo instance.If you have access to the fluo-conn.properties file that was used to configure your Fluo instance, youcan use it to build a FluoConfiguration object with all necessary properties:FluoConfiguration config = new FluoConfiguration(new File(&quot;fluo-conn.properties&quot;));config.setApplicationName(&quot;myapp&quot;);You can also create an empty FluoConfiguration object and set properties using Java:FluoConfiguration config = new FluoConfiguration();config.setInstanceZookeepers(&quot;localhost/fluo&quot;);config.setApplicationName(&quot;myapp&quot;);Once you have FluoConfiguration object, pass it to the newClient() method of FluoFactory tocreate a FluoClient:try(FluoClient client = FluoFactory.newClient(config)){  try (Transaction tx = client.newTransaction()) {    // read and write some data    tx.commit();  }  try (Snapshot snapshot = client.newSnapshot()) {    //read some data  }}It may help to reference the API javadocs while you are learning the Fluo API.Creating a Fluo observerTo create an observer, follow these steps:      Create one or more classes that extend Observer like the example below. It is a good idea touse slf4j for any logging in observers as slf4j supports multiple logging implementations.    public class InvertObserver implements Observer {  @Override  public void process(TransactionBase tx, Bytes row, Column col) throws Exception {    // read value    Bytes value = tx.get(row, col);    // invert row and value    tx.set(value, new Column(&quot;inv&quot;, &quot;data&quot;), row);  }}            Create a class that implements ObserverProvider like the example below.  The purpose of thisclass is associate a set Observers with columns that trigger the observers.  The class canregister multiple observers.    class AppObserverProvider implements ObserverProvider {  @Override  public void provide(Registry reg, Context ctx) {    //setup InvertObserver to be triggered when the column obs:data is modified    reg.forColumn(new Column(&quot;obs&quot;, &quot;data&quot;), NotificationType.STRONG)      .useObserver(new InvertObserver());            //Observer is a Functional interface.  So Observers can be written as lambdas.    reg.forColumn(new Column(&quot;new&quot;,&quot;data&quot;), NotificationType.WEAK)      .useObserver((tx,row,col) -&amp;gt; {         Bytes combined = combineNewAndOld(tx,row);         tx.set(row, new Column(&quot;current&quot;,&quot;data&quot;), combined);       });  }}        Build a jar containing these classes. Put this jar and any other dependencies required for yourapplication in a directory. Set fluo.observer.init.dir in fluo-app.properties to the path ofthis directory. When your application is initialized, these jars will be loaded to HDFS to makethem accessible to all of your Fluo workers on the cluster.  Configure your Fluo application to use your observer provider by modifying the Application section offluo-app.properties. Set fluo.observer.provider to the observer provider class name.  Initialize your Fluo application as described in the next section.  During initialization Fluowill obtain the observed columns from the ObserverProvider and persist the columns in Zookeeper.These columns persisted in Zookeeper are used by transactions to know when to trigger observers.Next StepsInitialize your Fluo application before running it.",
      "url": " /docs/fluo/1.2/getting-started/create-application",
      "categories": "getting-started"
    },
  
    "docs-fluo-1-2-getting-started-design": {
      "title": "Design",
      "content"	 : "The diagram below provides an overview of Apache Fluo’s design.BackgroundThe design of Apache Fluo is inspired by Google’s Percolator which is described asbeing used to populate Google’s search index.Fluo ApplicationA Fluo application maintains a large scale computation using a series of small transactionalupdates. Fluo applications store their data in a Fluo table which has a similar structure (row,column, value) to an Accumulo table except that a Fluo table has no timestamps. A Fluo tableis implemented using an Accumulo table. While you could scan the Accumulo table used to implementa Fluo table using an Accumulo client, you would read extra implementation-related data in additionto your data. Therefore, developers should only interact with the data in a Fluo table by writingFluo client or observer code:  Clients ingest data or interact with Fluo from external applications (REST services,crawlers, etc).  These are generally user started process that use the Fluo API.  Observers are user provided functions run by Fluo Workers that execute transactions in response to notifications. Notifications are set by Fluo transactions, executing in a client or observer, when a requested column is modified.Multiple Fluo applications can run on a cluster at the same time. Fluo applicationsconsist of an oracle process and a configurable number of worker processes:  The Oracle process allocates timestamps for transactions. While only one Oracle is required,Fluo can be configured to run extra Oracles that can take over if the primary Oracle fails.  Worker processes run user code (called observers) that perform transactions. All workers run the same observers. The number of worker instances are configured to handle the processing workload.Fluo DependenciesFluo requires the following software to be running on the cluster:  Accumulo - Fluo stores its data in Accumulo and uses Accumulo’s conditional mutations fortransactions.  Hadoop - Each Fluo application run its oracle and worker processes as Hadoop YARNapplications. HDFS is also required for Accumulo.  Zookeeper - Fluo stores its metadata and state information in Zookeeper. Zookeeper is alsorequired for Accumulo.",
      "url": " /docs/fluo/1.2/getting-started/design",
      "categories": "getting-started"
    },
  
    "docs-fluo-1-2-getting-started-install-fluo": {
      "title": "Install Fluo",
      "content"	 : "Instructions for installing Apache Fluo and starting a Fluo application on a cluster whereAccumulo, Hadoop &amp;amp; Zookeeper are running.  If you need help setting up these dependencies, see therelated projects page for external projects that may help.RequirementsBefore you install Fluo, the following software must be installed and running on your local machineor cluster:            Software      Recommended Version      Minimum Version                  Accumulo      1.8.1      1.7.0              Hadoop      2.7.5      2.6.0              Zookeeper      3.4.11                     Java      JDK 8      JDK 8      Obtain a distributionBefore you can install Fluo, you will need to obtain a distribution tarball. It is recommended thatyou download the latest release. You can also build a distribution from the main branch by followingthese steps which create a tarball in modules/distribution/target:git clone https://github.com/apache/fluo.gitcd fluo/mvn packageInstall FluoAfter you obtain a Fluo distribution tarball, follow these steps to install Fluo.      Choose a directory with plenty of space and untar the distribution:    tar -xvzf fluo-1.2.0-bin.tar.gzcd fluo-1.2.0        The distribution contains a fluo script in bin/ that administers Fluo and thefollowing configuration files in conf/:                            Configuration file          Description                                      fluo-env.sh          Configures classpath for fluo script. Required for all commands.                          fluo-conn.properties          Configures connection to Fluo. Required for all commands.                          fluo-app.properties          Template for configuration file passed to fluo init when initializing Fluo application.                          log4j.properties          Configures logging                          fluo.properties.deprecated          Deprecated Fluo configuration file. Replaced by fluo-conn.properties and fluo-app.properties                          Configure fluo-env.sh to set up your classpath using jars from the versions of Hadoop, Accumulo, andZookeeper that you are using. Choose one of the two ways below to make these jars available to Fluo:          Set HADOOP_PREFIX, ACCUMULO_HOME, and ZOOKEEPER_HOME in your environment or configurethese variables in fluo-env.sh. Fluo will look in these locations for jars.              Run ./lib/fetch.sh ahz to download Hadoop, Accumulo, and Zookeeper jars to lib/ahz andconfigure fluo-env.sh to look in this directory. By default, this command will download thedefault versions set in lib/ahz/pom.xml. If you are not using the default versions, you canoverride them:          ./lib/fetch.sh ahz -Daccumulo.version=1.7.2 -Dhadoop.version=2.7.2 -Dzookeeper.version=3.4.8                          Fluo needs more dependencies than what is available from Hadoop, Accumulo, and Zookeeper. Theseextra dependencies need to be downloaded to lib/ using the command below:     ./lib/fetch.sh extra      You are now ready to use the fluo script.Fluo command scriptThe Fluo command script is located at bin/fluo of your Fluo installation. All Fluo commands areinvoked by this script.Modify and add the following to your ~/.bashrc if you want to be able to execute the fluo scriptfrom any directory:export PATH=/path/to/fluo-1.2.0/bin:$PATHSource your .bashrc for the changes to take effect and test the scriptsource ~/.bashrcfluoRunning the script without any arguments prints a description of all commands../bin/fluoTuning AccumuloFluo will reread the same data frequently when it checks conditions on mutations. When Fluoinitializes a table it enables data caching to make this more efficient. However you may need toincrease the amount of memory available for caching in the tserver by increasingtserver.cache.data.size. Increasing this may require increasing the maximum tserver java heap sizein accumulo-env.sh.Fluo will run many client threads, will want to ensure the tablet server has enough threads. Shouldprobably increase the tserver.server.threads.minimum Accumulo setting.Using at least Accumulo 1.6.1 is recommended because multiple performance bugs were fixed.Next StepsCreate a Fluo application to run or use an example application.",
      "url": " /docs/fluo/1.2/getting-started/install-fluo",
      "categories": "getting-started"
    },
  
    "docs-fluo-1-2-getting-started-quick-start": {
      "title": "Quick Start",
      "content"	 : "Follow the steps below to run a Fluo application:      Install Fluo        Create a Fluo application or use an example application.        Initialize your Fluo application        Pick a method below to run your Fluo application          Run Fluo processes      Run Fluo in YARN      Run Fluo in Docker which enables running in Mesos and Kubernetes      ",
      "url": " /docs/fluo/1.2/getting-started/quick-start",
      "categories": "getting-started"
    },
  
    "docs-fluo-1-2-index": {
      "title": "Apache Fluo documentation",
      "content"	 : "",
      "url": " /docs/fluo/1.2/index",
      "categories": ""
    },
  
  
    "docs-fluo-recipes-1-2-getting-started-overview": {
      "title": "Overview",
      "content"	 : "Fluo Recipes are common code for Apache Fluo application developers. They build on theFluo API to offer additional functionality todevelopers. They are published separately from Fluo on their own release schedule.This allows Fluo Recipes to iterate and innovate faster than Fluo (which will maintaina more minimal API on a slower release cycle). Fluo Recipes offers code to implementcommon patterns on top of Fluo’s API.  It also offers glue code to external librarieslike Spark and Kryo.UsageThe Fluo Recipes project publishes multiple jars to Maven Central for each release.The fluo-recipes-core jar is the primary jar. It is where most recipes live and wherethey are placed by default if they have minimal dependencies beyond the Fluo API.Fluo Recipes with dependencies that bring in many transitive dependencies publishtheir own jar. For example, recipes that depend on Apache Spark are published in thefluo-recipes-spark jar.  If you don’t plan on using code in the fluo-recipes-sparkjar, you should avoid including it in your pom.xml to avoid a transitive dependency onSpark.Below is a sample Maven POM containing all possible Fluo Recipes dependencies:&amp;lt;properties&amp;gt;  &amp;lt;fluo-recipes.version&amp;gt;1.2.0&amp;lt;/fluo-recipes.version&amp;gt;&amp;lt;/properties&amp;gt;&amp;lt;dependencies&amp;gt;  &amp;lt;!-- Required. Contains recipes that are only depend on the Fluo API --&amp;gt;  &amp;lt;dependency&amp;gt;    &amp;lt;groupId&amp;gt;org.apache.fluo&amp;lt;/groupId&amp;gt;    &amp;lt;artifactId&amp;gt;fluo-recipes-core&amp;lt;/artifactId&amp;gt;    &amp;lt;version&amp;gt;${fluo-recipes.version}&amp;lt;/version&amp;gt;  &amp;lt;/dependency&amp;gt;  &amp;lt;!-- Optional. Serialization code that depends on Kryo --&amp;gt;  &amp;lt;dependency&amp;gt;    &amp;lt;groupId&amp;gt;org.apache.fluo&amp;lt;/groupId&amp;gt;    &amp;lt;artifactId&amp;gt;fluo-recipes-kryo&amp;lt;/artifactId&amp;gt;    &amp;lt;version&amp;gt;${fluo-recipes.version}&amp;lt;/version&amp;gt;  &amp;lt;/dependency&amp;gt;  &amp;lt;!-- Optional. Common code for using Fluo with Accumulo --&amp;gt;  &amp;lt;dependency&amp;gt;    &amp;lt;groupId&amp;gt;org.apache.fluo&amp;lt;/groupId&amp;gt;    &amp;lt;artifactId&amp;gt;fluo-recipes-accumulo&amp;lt;/artifactId&amp;gt;    &amp;lt;version&amp;gt;${fluo-recipes.version}&amp;lt;/version&amp;gt;  &amp;lt;/dependency&amp;gt;  &amp;lt;!-- Optional. Common code for using Fluo with Spark --&amp;gt;  &amp;lt;dependency&amp;gt;    &amp;lt;groupId&amp;gt;org.apache.fluo&amp;lt;/groupId&amp;gt;    &amp;lt;artifactId&amp;gt;fluo-recipes-spark&amp;lt;/artifactId&amp;gt;    &amp;lt;version&amp;gt;${fluo-recipes.version}&amp;lt;/version&amp;gt;  &amp;lt;/dependency&amp;gt;  &amp;lt;!-- Optional. Common code for writing Fluo integration tests --&amp;gt;  &amp;lt;dependency&amp;gt;    &amp;lt;groupId&amp;gt;org.apache.fluo&amp;lt;/groupId&amp;gt;    &amp;lt;artifactId&amp;gt;fluo-recipes-test&amp;lt;/artifactId&amp;gt;    &amp;lt;version&amp;gt;${fluo-recipes.version}&amp;lt;/version&amp;gt;    &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;  &amp;lt;/dependency&amp;gt;&amp;lt;/dependencies&amp;gt;",
      "url": " /docs/fluo-recipes/1.2/getting-started/overview",
      "categories": "getting-started"
    },
  
    "docs-fluo-recipes-1-2-index": {
      "title": "Apache Fluo documentation",
      "content"	 : "",
      "url": " /docs/fluo-recipes/1.2/index",
      "categories": ""
    },
  
    "docs-fluo-recipes-1-2-recipes-accumulo-export": {
      "title": "Accumulo Export",
      "content"	 : "BackgroundThe Export Queue Recipe provides a generic foundation for building export mechanism to anyexternal data store. The AccumuloExporter provides an Exporter for writing toAccumulo. AccumuloExporter is located in the fluo-recipes-accumulo module and provides thefollowing functionality:  Safely batches writes to Accumulo made by multiple transactions exporting data.  Stores Accumulo connection information in Fluo configuration, making it accessible by ExportObservers running on other nodes.  Provides utility code that make it easier and shorter to code common Accumulo export patterns.Example UseExporting to Accumulo is easy. Follow the steps below:      First, implement AccumuloTranslator.  Your implementation translates exportedobjects to Accumulo Mutations. For example, the SimpleTranslator class below translates Stringkey/values and into mutations for Accumulo.  This step is optional, a lambda couldbe used in step 3 instead of creating a class.    public class SimpleTranslator implements AccumuloTranslator&amp;lt;String,String&amp;gt; {  @Override  public void translate(SequencedExport&amp;lt;String, String&amp;gt; export, Consumer&amp;lt;Mutation&amp;gt; consumer) {    Mutation m = new Mutation(export.getKey());    m.put(&quot;cf&quot;, &quot;cq&quot;, export.getSequence(), export.getValue());    consumer.accept(m);  }}            Configure an ExportQueue and the export table prior to initializing Fluo.    FluoConfiguration fluoConfig = ...;String instance =       // Name of accumulo instance exporting toString zookeepers =     // Zookeepers used by Accumulo instance exporting toString user =           // Accumulo username, user that can write to exportTableString password =       // Accumulo user passwordString exportTable =    // Name of table to export to// Set properties for table to export to in Fluo app configuration.AccumuloExporter.configure(EXPORT_QID).instance(instance, zookeepers)    .credentials(user, password).table(exportTable).save(fluoConfig);// Set properties for export queue in Fluo app configurationExportQueue.configure(EXPORT_QID).keyType(String.class).valueType(String.class)    .buckets(119).save(fluoConfig);// Initialize Fluo using fluoConfig            In the applications ObserverProvider, register an observer that will process exports and writethem to Accumulo using AccumuloExporter.  Also, register observers that add to the exportqueue.    public class MyObserverProvider implements ObserverProvider {  @Override  public void provide(Registry obsRegistry, Context ctx) {    SimpleConfiguration appCfg = ctx.getAppConfiguration();    ExportQueue&amp;lt;String, String&amp;gt; expQ = ExportQueue.getInstance(EXPORT_QID, appCfg);    // Register observer that will processes entries on export queue and write them to the Accumulo    // table configured earlier. SimpleTranslator from step 1 is passed here, could have used a    // lambda instead.    expQ.registerObserver(obsRegistry,        new AccumuloExporter&amp;lt;&amp;gt;(EXPORT_QID, appCfg, new SimpleTranslator()));    // An example observer created using a lambda that adds to the export queue.    obsRegistry.forColumn(OBS_COL, WEAK).useObserver((tx,row,col) -&amp;gt; {      // Read some data and do some work      // Add results to export queue      String key =    // key that identifies export      String value =  // object to export      expQ.add(tx, key, value);    });  }}      Other use casesThe getTranslator() method in AccumuloReplicator creates a specialized AccumuloTranslator for replicating a Fluo table to Accumulo.",
      "url": " /docs/fluo-recipes/1.2/recipes/accumulo-export",
      "categories": "recipes"
    },
  
    "docs-fluo-recipes-1-2-recipes-combine-queue": {
      "title": "Combine Queue",
      "content"	 : "BackgroundWhen many transactions try to modify the same keys, collisions will occur.  Too many collisionscause transactions to fail and throughput to nose dive.  For example, consider phrasecountwhich has many transactions processing documents.  Each transaction counts the phrases in a documentand then updates global phrase counts.  Since transaction attempts to update many phrases, the probability of collisions is high.SolutionThe combine queue recipe provides a reusable solution for updating many keys whileavoiding collisions.  The recipe also organizes updates into batches in order to improve throughput.This recipes queues updates to keys for other transactions to process. In the phrase count exampletransactions processing documents queue updates, but do not actually update the counts.  Below is anexample of computing phrasecounts using this recipe.  TX1 queues +1 update  for phrase we want lambdas now  TX2 queues +1 update  for phrase we want lambdas now  TX3 reads the updates and current value for the phrase we want lambdas now.  There is no current value and the updates sum to 2, so a new value of 2 is written.  TX4 queues +2 update  for phrase we want lambdas now  TX5 queues -1 update  for phrase we want lambdas now  TX6 reads the updates and current value for the phrase we want lambdas now.  The current value is 2 and the updates sum to 1, so a new value of 3 is written.Transactions processing updates have the ability to make additional updates.For example in addition to updating the current value for a phrase, the newvalue could also be placed on an export queue to update an external database.BucketsA simple implementation of this recipe would have an update queue for each key.  However theimplementation is slightly more complex.  Each update queue is in a bucket and transactions processall of the updates in a bucket.  This allows more efficient processing of updates for the followingreasons :  When updates are queued, notifications are made per bucket(instead of per a key).  The transaction doing the update can scan the entire bucket reading updates, this avoids a seek for each key being updated.  Also the transaction can request a batch lookup to get the current value of all the keys being updated.  Any additional actions taken on update (like adding something to an export queue) can also be batched.  Data is organized to make reading exiting values for keys in a bucket more efficient.Which bucket a key goes to is decided using hash and modulus so that multiple updates for a key goto the same bucket.The initial number of tablets to create when applying table optimizations can be controlled bysetting the buckets per tablet option when configuring a Combine Queue.  For example if youhave 20 tablet servers and 1000 buckets and want 2 tablets per tserver initially then set bucketsper tablet to 1000/(2*20)=25.Example UseThe following code snippets show how to use this recipe for wordcount.  The first step is toconfigure it before initializing Fluo.  When initializing an ID is needed.  This ID is used in twoways.  First, the ID is used as a row prefix in the table.  Therefore nothing else should use thatrow range in the table.  Second, the ID is used in generating configuration keys.The following snippet shows how to configure a combine queue.FluoConfiguration fluoConfig = ...;// Set application properties for the combine queue.  These properties are read later by// the observers running on each worker.CombineQueue.configure(WcObserverProvider.ID)    .keyType(String.class).valueType(Long.class).buckets(119).save(fluoConfig);fluoConfig.setObserverProvider(WcObserverProvider.class);// initialize Fluo using fluoConfigAssume the following observer is triggered when a documents is updated.  It examines newand old document content and determines changes in word counts.  These changes are pushed to acombine queue.public class DocumentObserver implements StringObserver {  // word count combine queue  private CombineQueue&amp;lt;String, Long&amp;gt; wccq;  public static final Column NEW_COL = new Column(&quot;content&quot;, &quot;new&quot;);  public static final Column CUR_COL = new Column(&quot;content&quot;, &quot;current&quot;);  public DocumentObserver(CombineQueue&amp;lt;String, Long&amp;gt; wccq) {    this.wccq = wccq;  }  @Override  public void process(TransactionBase tx, String row, Column col) {    Preconditions.checkArgument(col.equals(NEW_COL));    String newContent = tx.gets(row, NEW_COL);    String currentContent = tx.gets(row, CUR_COL, &quot;&quot;);    Map&amp;lt;String, Long&amp;gt; newWordCounts = getWordCounts(newContent);    Map&amp;lt;String, Long&amp;gt; currentWordCounts = getWordCounts(currentContent);    // determine changes in word counts between old and new document content    Map&amp;lt;String, Long&amp;gt; changes = calculateChanges(newWordCounts, currentWordCounts);    // queue updates to word counts for processing by other transactions    wccq.add(tx, changes);    // update the current content and delete the new content    tx.set(row, CUR_COL, newContent);    tx.delete(row, NEW_COL);  }  private static Map&amp;lt;String, Long&amp;gt; getWordCounts(String doc) {    // TODO extract words from doc  }  private static Map&amp;lt;String, Long&amp;gt; calculateChanges(Map&amp;lt;String, Long&amp;gt; newCounts,      Map&amp;lt;String, Long&amp;gt; currCounts) {    Map&amp;lt;String, Long&amp;gt; changes = new HashMap&amp;lt;&amp;gt;();    // guava Maps class    MapDifference&amp;lt;String, Long&amp;gt; diffs = Maps.difference(currCounts, newCounts);    // compute the diffs for words that changed    changes.putAll(Maps.transformValues(diffs.entriesDiffering(),        vDiff -&amp;gt; vDiff.rightValue() - vDiff.leftValue()));    // add all new words    changes.putAll(diffs.entriesOnlyOnRight());    // subtract all words no longer present    changes.putAll(Maps.transformValues(diffs.entriesOnlyOnLeft(), l -&amp;gt; l * -1));    return changes;  }}Each combine queue has two extension points, a combiner and a changeobserver.  The combine queue configures a Fluo observer to process queuedupdates.  When processing updates the two extension points are called.  The code below showshow to use these extension points.A change observer can do additional processing when a batch of key values are updated.  Belowupdates are queued for export to an external database.  The export is given the new and old valueallowing it to delete the old value if needed.public class WcObserverProvider implements ObserverProvider {  public static final String ID = &quot;wc&quot;;  @Override  public void provide(Registry obsRegistry, Context ctx) {    ExportQueue&amp;lt;String, MyDatabaseExport&amp;gt; exportQ = createExportQueue(ctx);    // Create a combine queue for computing word counts.    CombineQueue&amp;lt;String, Long&amp;gt; wcMap = CombineQueue.getInstance(ID, ctx.getAppConfiguration());    // Register observer that updates the Combine Queue    obsRegistry.forColumn(DocumentObserver.NEW_COL, STRONG).useObserver(new DocumentObserver(wcMap));    // Used to join new and existing values for a key. The lambda sums all values and returns    // Optional.empty() when the sum is zero. Returning Optional.empty() causes the key/value to be    // deleted. Could have used the built in SummingCombiner.    Combiner&amp;lt;String, Long&amp;gt; combiner = input -&amp;gt; input.stream().reduce(Long::sum).filter(l -&amp;gt; l != 0);    // Called when the value of a key changes. The lambda exports these changes to an external    // database. Make sure to read ChangeObserver&#39;s javadoc.    ChangeObserver&amp;lt;String, Long&amp;gt; changeObs = (tx, changes) -&amp;gt; {      for (Change&amp;lt;String, Long&amp;gt; update : changes) {        String word = update.getKey();        Optional&amp;lt;Long&amp;gt; oldVal = update.getOldValue();        Optional&amp;lt;Long&amp;gt; newVal = update.getNewValue();        // Queue an export to let an external database know the word count has changed.        exportQ.add(tx, word, new MyDatabaseExport(oldVal, newVal));      }    };    // Register observer that handles updates to the CombineQueue. This observer will use the    // combiner and valueObserver.    wcMap.registerObserver(obsRegistry, combiner, changeObs);  }}GuaranteesThis recipe makes two important guarantees about updates for a key when itcalls process() on a ChangeObserver.  The new value reported for an update will be derived from combining allupdates that were committed before the transaction that’s processing updatesstarted.  The implementation may have to make multiple passes over queuedupdates to achieve this.  In the situation where TX1 queues a +1 and laterTX2 queues a -1 for the same key, there is no need to worry about only seeingthe -1 processed.  A transaction that started processing updates after TX2committed would process both.  The old value will always be what was reported as the new value in theprevious transaction that called ChangeObserver.process().",
      "url": " /docs/fluo-recipes/1.2/recipes/combine-queue",
      "categories": "recipes"
    },
  
    "docs-fluo-recipes-1-2-recipes-export-queue": {
      "title": "Export Queue",
      "content"	 : "BackgroundFluo is not suited for servicing low latency queries for two reasons. First, the implementation oftransactions are designed for throughput.  To get throughput, transactions recover lazily fromfailures and may wait on other transactions.  Both of these design decisions canlead to delays of individual transactions, but do not negatively impact throughput.   The secondreason is that Fluo observers executing transactions will likely cause a large number of randomaccesses.  This could lead to high response time variability for an individual random access.  Thisvariability would not impede throughput but would impede the goal of low latency.One way to make data transformed by Fluo available for low latency queries isto export that data to another system.  For example Fluo could run oncluster A, continually transforming a large data set, and exporting data toAccumulo tables on cluster B.  The tables on cluster B would service userqueries.  Fluo Recipes has built in support for exporting to Accumulo,however this recipe can be used to export to systems other than Accumulo, likeRedis, Elasticsearch, MySQL, etc.Exporting data from Fluo is easy to get wrong which is why this recipe exists.To understand what can go wrong consider the following example observertransaction.public class MyObserver implements StringObserver {    static final Column UPDATE_COL = new Column(&quot;meta&quot;, &quot;numUpdates&quot;);    static final Column COUNTER_COL = new Column(&quot;meta&quot;, &quot;counter1&quot;);    // represents a Query system external to Fluo that is updated by Fluo    QuerySystem querySystem;    @Override    public void process(TransactionBase tx, String row, Column col) {       int oldCount = Integer.parseInt(tx.gets(row, COUNTER_COL, &quot;0&quot;));       int numUpdates = Integer.parseInt(tx.gets(row, UPDATE_COL, &quot;0&quot;));       int newCount = oldCount + numUpdates;       tx.set(row, COUNTER_COL, &quot;&quot; + newCount);       tx.delete(row, UPDATE_COL);        //Build an inverted index in the query system, based on count from the        //meta:counter1 column in fluo.  Do this by creating rows for the        //external query system based on the count.        String oldCountRow = String.format(&quot;%06d&quot;, oldCount);        String newCountRow = String.format(&quot;%06d&quot;, newCount);        //add a new entry to the inverted index        querySystem.insertRow(newCountRow, row);        //remove the old entry from the inverted index        querySystem.deleteRow(oldCountRow, row);    }}The above example would keep the external index up to date beautifully as longas the following conditions are met.  Threads executing transactions always complete successfully.  Only a single thread ever responds to a notification.However these conditions are not guaranteed by Fluo.  Multiple threads mayattempt to process a notification concurrently (only one may succeed).  Also atany point in time a transaction may fail (for example the computer executing itmay reboot).   Both of these problems will occur and will lead to corruption ofthe external index in the example.  The inverted index and Fluo  will becomeinconsistent.  The inverted index will end up with multiple entries (that arenever cleaned up) for single entity even though the intent is to only have one.The root of the problem in the example above is that its exporting uncommitteddata.  There is no guarantee that setting the column &amp;lt;row&amp;gt;:meta:counter1 tonewCount will succeed until the transaction is successfully committed.However, newCountRow is derived from newCount and written to the external querysystem before the transaction is committed (Note : for observers, thetransaction is committed by the framework after process(...) is called).  Soif the transaction fails, the next time it runs it could compute a completelydifferent value for newCountRow (and it would not delete what was written by thefailed transaction).SolutionThe simple solution to the problem of exporting uncommitted data is to onlyexport committed data.  There are multiple ways to accomplish this.  Thisrecipe offers a reusable implementation of one method.  This recipe has thefollowing elements:  An export queue that transactions can add key/values to.  Only if the transaction commits successfully will the key/value end up in the queue.  A Fluo application can have multiple export queues, each one must have a unique id.  When a key/value is added to the export queue, its given a sequence number.  This sequence number is based on the transactions start timestamp.  Each export queue is configured with an observer that processes key/values that were successfully committed to the queue.  When key/values in an export queue are processed, they are deleted so the export queue does not keep any long term data.  Key/values in an export queue are placed in buckets.  This is done so that all of the updates in a bucket can be processed in a single transaction.  This allows an efficient implementation of this recipe in Fluo.  It can also lead to efficiency in a system being exported to, if the system can benefit from batching updates.  The number of buckets in an export queue is configurable.There are three requirements for using this recipe :  Must configure export queues before initializing a Fluo application.  Transactions adding to an export queue must get an instance of the queue using its unique QID.  Must create a class or lambda that implements Exporter in order to process exports.Example UseThis example shows how to incrementally build an inverted index in an external query system using anexport queue.  The class below is simple POJO used as the value for the export queue.class CountUpdate {  public int oldCount;  public int newCount;  public CountUpdate(int oc, int nc) {    this.oldCount = oc;    this.newCount = nc;  }}The following code shows how to configure an export queue.  This codemodifies the FluoConfiguration object with options needed for the export queue.This FluoConfiguration object should be used to initialize the fluoapplication.public class FluoApp {  // export queue id &quot;ici&quot; means inverted count index  public static final String EQ_ID = &quot;ici&quot;;  static final Column UPDATE_COL = new Column(&quot;meta&quot;, &quot;numUpdates&quot;);  static final Column COUNTER_COL = new Column(&quot;meta&quot;, &quot;counter1&quot;);  public static class AppObserverProvider implements ObserverProvider {    @Override    public void provide(Registry obsRegistry, Context ctx) {      ExportQueue&amp;lt;String, CountUpdate&amp;gt; expQ =          ExportQueue.getInstance(EQ_ID, ctx.getAppConfiguration());      // register observer that will queue data to export      obsRegistry.forColumn(UPDATE_COL, STRONG).useObserver(new MyObserver(expQ));      // register observer that will export queued data      expQ.registerObserver(obsRegistry, new CountExporter());    }  }  /**   * Call this method before initializing Fluo.   *   * @param fluoConfig the configuration object that will be used to initialize Fluo   */  public static void preInit(FluoConfiguration fluoConfig) {    // Set properties for export queue in Fluo app configuration    ExportQueue.configure(QUEUE_ID)        .keyType(String.class)        .valueType(CountUpdate.class)        .buckets(1009)        .bucketsPerTablet(10)        .save(getFluoConfiguration());    fluoConfig.setObserverProvider(AppObserverProvider.class);  }}Below is updated version of the observer from above that is now using an export queue.public class MyObserver implements StringObserver {  private ExportQueue&amp;lt;String, CountUpdate&amp;gt; exportQueue;  public MyObserver(ExportQueue&amp;lt;String, CountUpdate&amp;gt; exportQueue) {    this.exportQueue = exportQueue;  }  @Override  public void process(TransactionBase tx, String row, Column col) {    int oldCount = Integer.parseInt(tx.gets(row, FluoApp.COUNTER_COL, &quot;0&quot;));    int numUpdates = Integer.parseInt(tx.gets(row, FluoApp.UPDATE_COL, &quot;0&quot;));    int newCount = oldCount + numUpdates;    tx.set(row, FluoApp.COUNTER_COL, &quot;&quot; + newCount);    tx.delete(row, FluoApp.UPDATE_COL);    // Because the update to the export queue is part of the transaction,    // either the update to meta:counter1 is made and an entry is added to    // the export queue or neither happens.    exportQueue.add(tx, row, new CountUpdate(oldCount, newCount));  }}The export queue will call the accept() method on the class below to process entries queued forexport.  It is possible the call to accept() can fail part way through and/or be called multipletimes.  In the case of failures the export consumer will be called again with the same data.Its possible for the same export entry to be processed on multiple computers at different times.This can cause exports to arrive out of order.   The purpose of the sequence number is to helpsystems receiving out of order and redundant data.public class CountExporter implements Exporter&amp;lt;String, CountUpdate&amp;gt; {  // represents the external query system we want to update from Fluo  QuerySystem querySystem;  @Override  public void export(Iterator&amp;lt;SequencedExport&amp;lt;String, CountUpdate&amp;gt;&amp;gt; exports) {    BatchUpdater batchUpdater = querySystem.getBatchUpdater();    while (exports.hasNext()) {      SequencedExport&amp;lt;String, CountUpdate&amp;gt; export = exports.next();      String row = export.getKey();      CountUpdate uc = export.getValue();      long seqNum = export.getSequence();      String oldCountRow = String.format(&quot;%06d&quot;, uc.oldCount);      String newCountRow = String.format(&quot;%06d&quot;, uc.newCount);      // add a new entry to the inverted index      batchUpdater.insertRow(newCountRow, row, seqNum);      // remove the old entry from the inverted index      batchUpdater.deleteRow(oldCountRow, row, seqNum);    }    // flush all of the updates to the external query system    batchUpdater.close();  }}SchemaEach export queue stores its data in the Fluo table in a contiguous row range.This row range is defined by using the export queue id as a row prefix for alldata in the export queue.  So the row range defined by the export queue idshould not be used by anything else.All data stored in an export queue is transient. When an exportqueue is configured, it will recommend split points using the tableoptimization process.  The number of splits generatedby this process can be controlled by setting the number of buckets per tabletwhen configuring an export queue.ConcurrencyAdditions to the export queue will never collide.  If two transactions add thesame key at around the same time and successfully commit, then two entries withdifferent sequence numbers will always be added to the queue.  The sequencenumber is based on the start timestamp of the transactions.If the key used to add items to the export queue is deterministically derivedfrom something the transaction is writing to, then that will cause a collision.For example consider the following interleaving of two transactions adding tothe same export queue in a manner that will collide. Note, TH1 is shorthand forthread 1, ek() is a function the creates the export key, and ev() is a functionthat creates the export value.  TH1 : key1 = ek(row1,fam1:qual1)  TH1 : val1 = ev(tx1.get(row1,fam1:qual1), tx1.get(rowA,fam1:qual2))  TH1 : exportQueueA.add(tx1, key1, val1)  TH2 : key2 = ek(row1,fam1:qual1)  TH2 : val2 = ev(tx2.get(row1,fam1:qual1), tx2.get(rowB,fam1:qual2))  TH2 : exportQueueA.add(tx2, key2, val2)  TH1 : tx1.set(row1,fam1:qual1, val1)  TH2 : tx2.set(row1,fam1:qual1, val2)In the example above only one transaction will succeed because both are settingrow1 fam1:qual1.  Since adding to the export queue is part of thetransaction, only the transaction that succeeds will add something to thequeue.  If the function ek() in the example is deterministic, then bothtransactions would have been trying to add the same key to the export queue.With the above method, we know that transactions adding entries to the queue forthe same key must have executed serially. Knowing that transactions whichadded the same key did not overlap in time makes reasoning about those exportentries very simple.The example below is a slight modification of the example above.  In thisexample both transactions will successfully add entries to the queue using thesame key.  Both transactions succeed because they are writing to differentcells (rowB fam1:qual2 and rowA fam1:qual2).  This approach makes it moredifficult to reason about export entries with the same key, because thetransactions adding those entries could have overlapped in time.  This is anexample of write skew mentioned in the Percolator paper.  TH1 : key1 = ek(row1,fam1:qual1)  TH1 : val1 = ev(tx1.get(row1,fam1:qual1), tx1.get(rowA,fam1:qual2))  TH1 : exportQueueA.add(tx1, key1, val1)  TH2 : key2 = ek(row1,fam1:qual1)  TH2 : val2 = ev(tx2.get(row1,fam1:qual1), tx2.get(rowB,fam1:qual2))  TH2 : exportQueueA.add(tx2, key2, val2)  TH1 : tx1.set(rowA,fam1:qual2, val1)  TH2 : tx2.set(rowB,fam1:qual2, val2)",
      "url": " /docs/fluo-recipes/1.2/recipes/export-queue",
      "categories": "recipes"
    },
  
    "docs-fluo-recipes-1-2-recipes-recording-tx": {
      "title": "Recording Transaction",
      "content"	 : "A RecordingTransaction is an implementation of Transaction that logs all transaction operations(i.e GET, SET, or DELETE) to a TxLog object for later uses such as exporting data.  The code belowshows how a RecordingTransaction is created by wrapping a Transaction object:RecordingTransactionBase rtx = RecordingTransactionBase.wrap(tx);A predicate function can be passed to wrap method to select which log entries to record.  The codebelow only records log entries whose column family is meta:RecordingTransactionBase rtx = RecordingTransactionBase.wrap(tx,                               le -&amp;gt; le.getColumn().getFamily().toString().equals(&quot;meta&quot;));After creating a RecordingTransaction, users can use it as they would use a Transaction object.Bytes value = rtx.get(Bytes.of(&quot;r1&quot;), new Column(&quot;cf1&quot;, &quot;cq1&quot;));While SET or DELETE operations are always recorded to the log, GET operations are only recorded if avalue was found at the requested row/column.  Also, if a GET method returns an iterator, only the GEToperations that are retrieved from the iterator are logged.  GET operations are logged as they arenecessary if you want to determine the changes made by the transaction.When you are done operating on the transaction, you can retrieve the TxLog using the following code:TxLog myTxLog = rtx.getTxLog()Below is example code of how a RecordingTransaction can be used in an observer to record all operationsperformed by the transaction in a TxLog.  In this example, a GET (if data exists) and SET operationwill be logged.  This TxLog can be added to an export queue and later used to export updates from Fluo.public class MyObserver extends AbstractObserver {    private static final TYPEL = new TypeLayer(new StringEncoder());        private ExportQueue&amp;lt;Bytes, TxLog&amp;gt; exportQueue;    @Override    public void process(TransactionBase tx, Bytes row, Column col) {        // create recording transaction (rtx)        RecordingTransactionBase rtx = RecordingTransactionBase.wrap(tx);                // use rtx to create a typed transaction &amp;amp; perform operations        TypedTransactionBase ttx = TYPEL.wrap(rtx);        int count = ttx.get().row(row).fam(&quot;meta&quot;).qual(&quot;counter1&quot;).toInteger(0);        ttx.mutate().row(row).fam(&quot;meta&quot;).qual(&quot;counter1&quot;).set(count+1);                // when finished performing operations, retrieve transaction log        TxLog txLog = rtx.getTxLog()        // add txLog to exportQueue if not empty        if (!txLog.isEmpty()) {          //do not pass rtx to exportQueue.add()          exportQueue.add(tx, row, txLog)        }    }}",
      "url": " /docs/fluo-recipes/1.2/recipes/recording-tx",
      "categories": "recipes"
    },
  
    "docs-fluo-recipes-1-2-recipes-row-hasher": {
      "title": "Row Hash Prefix",
      "content"	 : "BackgroundTransactions are implemented in Fluo using conditional mutations.  Conditionalmutations require server side processing on tservers.  If data is not spreadevenly, it can cause some tservers to execute more conditional mutations thanothers.  These tservers doing more work can become a bottleneck.  Most realworld data is not uniform and can cause this problem.Before the Fluo Webindex example started using this recipe it sufferedfrom this problem.  The example was using reverse dns encoded URLs for row keyslike p:com.cnn/story1.html.  This made certain portions of the table morepopular, which in turn made some tservers do much more work.  This unevendistribution of work lead to lower throughput and uneven performance.  Usingthis recipe made those problems go away.SolutionThis recipe provides code to help add a hash of the row as a prefix of the row.Using this recipe rows are structured like the following.&amp;lt;prefix&amp;gt;:&amp;lt;fixed len row hash&amp;gt;:&amp;lt;user row&amp;gt;The recipe also provides code to help generate split points and configurebalancing of the prefix.Example Useimport org.apache.fluo.api.config.FluoConfiguration;import org.apache.fluo.api.data.Bytes;import org.apache.fluo.recipes.core.data.RowHasher;public class RowHasherExample {  private static final RowHasher PAGE_ROW_HASHER = new RowHasher(&quot;p&quot;);  // Provide one place to obtain row hasher.  public static RowHasher getPageRowHasher() {    return PAGE_ROW_HASHER;  }  public static void main(String[] args) {    RowHasher pageRowHasher = getPageRowHasher();    String revUrl = &quot;org.wikipedia/accumulo&quot;;    // Add a hash prefix to the row. Use this hashedRow in your transaction    Bytes hashedRow = pageRowHasher.addHash(revUrl);    System.out.println(&quot;hashedRow      : &quot; + hashedRow);    // Remove the prefix. This can be used by transactions dealing with the hashed row.    Bytes orig = pageRowHasher.removeHash(hashedRow);    System.out.println(&quot;orig           : &quot; + orig);    // Generate table optimizations for the recipe. This can be called when setting up an    // application that uses a hashed row.    int numTablets = 20;    // The following code would normally be called before initializing Fluo. This code    // registers table optimizations for your prefix+hash.    FluoConfiguration conf = new FluoConfiguration();    RowHasher.configure(conf, PAGE_ROW_HASHER.getPrefix(), numTablets);    // Normally you would not call the following code, it would be called automatically for you by    // TableOperations.optimizeTable(). Calling this code here to show what table optimization will    // be generated.    TableOptimizations tableOptimizations = new RowHasher.Optimizer()        .getTableOptimizations(PAGE_ROW_HASHER.getPrefix(), conf.getAppConfiguration());    System.out.println(&quot;Balance config : &quot; + tableOptimizations.getTabletGroupingRegex());    System.out.println(&quot;Splits         : &quot;);    tableOptimizations.getSplits().forEach(System.out::println);    System.out.println();  }}The example program above prints the following.hashedRow      : p:1yl0:org.wikipedia/accumuloorig           : org.wikipedia/accumuloBalance config : (Qp:E).*Splits         : p:1sstp:3llmp:5eefp:7778p:9001p:assup:cllnp:eeegp:g779p:i002p:jssvp:lllop:neehp:p77ap:r003p:ssswp:ullpp:weeip:y77bp:~The split points are used to create tablets in the Accumulo table used by Fluo.Data and computation will spread very evenly across these tablets.  TheBalancing config will spread the tablets evenly across the tablet servers,which will spread the computation evenly. See the table optimizationsdocumentation for information on how to apply the optimizations.",
      "url": " /docs/fluo-recipes/1.2/recipes/row-hasher",
      "categories": "recipes"
    },
  
    "docs-fluo-recipes-1-2-tools-serialization": {
      "title": "Serializing Data",
      "content"	 : "Various Fluo Recipes deal with POJOs and need to serialize them.  Theserialization mechanism is configurable and defaults to using Kryo.Custom SerializationIn order to use a custom serialization method, two steps need to be taken.  Thefirst step is to implement SimpleSerializer.  The second step is toconfigure Fluo Recipes to use the custom implementation.  This needs to be donebefore initializing Fluo.  Below is an example of how to do this.FluoConfiguration fluoConfig = ...;//assume MySerializer implements SimpleSerializerSimpleSerializer.setSetserlializer(fluoConfig, MySerializer.class);//initialize Fluo using fluoConfigKryo FactoryIf using the default Kryo serializer implementation, then creating aKryoFactory implementation can lead to smaller serialization size.  When Kryoserializes an object graph, it will by default include the fully qualifiednames of the classes in the serialized data.  This can be avoided byregistering classes that will be serialized.  Registration is done bycreating a KryoFactory and then configuring Fluo Recipes to use it.   Theexample below shows how to do this.For example assume the POJOs named Node and Edge will be serialized andneed to be registered with Kryo.  This could be done by creating a KryoFactorylike the following.package com.foo;import com.esotericsoftware.kryo.Kryo;import com.esotericsoftware.kryo.pool.KryoFactory;import com.foo.data.Edge;import com.foo.data.Node;public class MyKryoFactory implements KryoFactory {  @Override  public Kryo create() {    Kryo kryo = new Kryo();        //Explicitly assign each class a unique id here to ensure its stable over    //time and in different environments with different dependencies.    kryo.register(Node.class, 9);    kryo.register(Edge.class, 10);        //instruct kryo that these are the only classes we expect to be serialized    kryo.setRegistrationRequired(true);        return kryo;  }}Fluo Recipes must be configured to use this factory.  The following code showshow to do this.FluoConfiguration fluoConfig = ...;KryoSimplerSerializer.setKryoFactory(fluoConfig, MyKryoFactory.class);//initialize Fluo using fluoConfig",
      "url": " /docs/fluo-recipes/1.2/tools/serialization",
      "categories": "tools"
    },
  
    "docs-fluo-recipes-1-2-tools-spark": {
      "title": "Spark Helper",
      "content"	 : "Fluo Recipes has some helper code for Apache Spark.  Most of the helper code is currentlyrelated to bulk importing data into Accumulo.  This is useful for initializing a new Fluo table withhistorical data via Spark.  The Spark helper code is found in the fluo-recipes-spark module.For information on using Spark to load data into Fluo, check out this blog post.If you know of other Spark+Fluo integration code that would be useful, then please consider openingan issue.",
      "url": " /docs/fluo-recipes/1.2/tools/spark",
      "categories": "tools"
    },
  
    "docs-fluo-recipes-1-2-tools-table-optimization": {
      "title": "Table Optimization",
      "content"	 : "BackgroundRecipes may need to make Accumulo specific table modifications for optimalperformance.  Configuring the Accumulo tablet balancer and adding splits aretwo optimizations that are currently done.  Offering a standard way to do theseoptimizations makes it easier to use recipes correctly.  These optimizationsare optional.  You could skip them for integration testing, but would probablywant to use them in production.Java ExampleFluoConfiguration fluoConf = ...//export queue configure method will return table optimizations it would like madeExportQueue.configure(fluoConf, ...);//CollisionFreeMap.configure() will return table optimizations it would like madeCollisionFreeMap.configure(fluoConf, ...);//configure optimizations for a prefixed hash range of a tableRowHasher.configure(fluoConf, ...);//initialize FluoFluoFactory.newAdmin(fluoConf).initialize(...)//Automatically optimize the Fluo table for all configured recipesTableOperations.optimizeTable(fluoConf);TableOperations is provided in the Accumulo module of Fluo Recipes.Command ExampleFluo Recipes provides an easy way to optimize a Fluo table for configuredrecipes from the command line.  This should be done after configuring recipesand initializing Fluo.  Below are example command for initializing in this way.#create application fluo new app1#configure application#initialize Fluofluo init app1#optimize table for all configured recipesfluo exec app1 org.apache.fluo.recipes.accumulo.cmds.OptimizeTableTable optimization registryRecipes register themselves by calling TableOptimizations.registerOptimization().  Anyone can usethis mechanism, its not limited to use by existing recipes.",
      "url": " /docs/fluo-recipes/1.2/tools/table-optimization",
      "categories": "tools"
    },
  
    "docs-fluo-recipes-1-2-tools-testing": {
      "title": "Testing",
      "content"	 : "Fluo includes MiniFluo which makes it possible to write an integration test thatruns against a real Fluo instance.  Fluo Recipes provides the following utilitycode for writing an integration test.  FluoITHelper A class with utility methods for comparing expected data with whats in Fluo.  AccumuloExportITBase A base class for writing an integration test that exports data from Fluo to an external Accumulo table.",
      "url": " /docs/fluo-recipes/1.2/tools/testing",
      "categories": "tools"
    },
  
    "docs-fluo-recipes-1-2-tools-transient": {
      "title": "Transient Data",
      "content"	 : "BackgroundSome recipes store transient data in a portion of the Fluo table.  Transientdata is data that is continually being added and deleted.  Also these transientdata ranges contain no long term data.  The way Fluo works, when data isdeleted a delete marker is inserted but the data is actually still there.  Overtime these transient ranges of the table will have a lot more delete markersthan actual data if nothing is done.  If nothing is done, then processingtransient data will get increasingly slower over time.These deleted markers can be cleaned up by forcing Accumulo to compact theFluo table, which will run Fluo’s garbage collection iterator. However,compacting the entire table to clean up these ranges within a table isoverkill. Alternatively,  Accumulo supports compacting ranges of a table.   Soa good solution to the delete marker problem is to periodically compact justthe transient ranges.Fluo Recipes provides helper code to deal with transient data ranges in astandard way.Registering Transient RangesRecipes like Export Queue will automatically registertransient ranges when configured.  If you would like to register your owntransient ranges, use TransientRegistry.  Below is a simple example ofusing this.FluoConfiguration fluoConfig = ...;TransientRegistry transientRegistry = new TransientRegistry(fluoConfig.getAppConfiguration());transientRegistry.addTransientRange(new RowRange(startRow, endRow));//Initialize Fluo using fluoConfig. This will store the registered ranges in//zookeeper making them available on any node later.Compacting Transient RangesAlthough you may never need to register transient ranges directly, you willneed to periodically compact transient ranges if using a recipe that registersthem.  Using TableOperations this can be done with one line of Java codelike the following.FluoConfiguration fluoConfig = ...;TableOperations.compactTransient(fluoConfig);Fluo recipes provides an easy way to compact transient ranges from the command line using the fluo exec command as follows:fluo exec &amp;lt;app name&amp;gt; org.apache.fluo.recipes.accumulo.cmds.CompactTransient [&amp;lt;interval&amp;gt; [&amp;lt;multiplier&amp;gt;]]If no arguments are specified the command will call compactTransient() once.If &amp;lt;interval&amp;gt; is specified the command will run forever compacting transientranges sleeping &amp;lt;interval&amp;gt; seconds between compacting each transient ranges.In the case where Fluo is backed up in processing data a transient range couldhave a lot of data queued and compacting it too frequently would becounterproductive.  To avoid this the CompactTransient command will considerthe time it took to compact a range when deciding when to compact that rangenext.  This is where the &amp;lt;multiplier&amp;gt; argument comes in, the time to sleepbetween compactions of a range is determined as follows.  If not specified, themultiplier defaults to 3.sleepTime = Math.max(compactTime * multiplier, interval);For example assume a Fluo application has two transient ranges.  Also assumeCompactTransient is run with an interval of 600 and a multiplier of 10.  If thefirst range takes 20 seconds to compact, then it will be compacted again in 600seconds.  If the second range takes 80 seconds to compact, then it will becompacted again in 800 seconds.",
      "url": " /docs/fluo-recipes/1.2/tools/transient",
      "categories": "tools"
    },
  
  
    "blog-2019-09-30-scan-executors": {
      "title": "How Fluo Leveraged Scan Executors",
      "content"	 : "Accumulo 2.0 introduced Scan Executors giving control over processing ofscans in Accumulo tablet servers. Fluo has a good use case for scan executors:notification scans.  Fluo workers continually scan for notifications to findtransactions to execute. All workers continually scanning for notificationsputs load on Accumulo tablet servers which could negatively impacttransactions.  Scan executors provides a way to limit this load.Fluo utilizes this feature by setting scan hints for notification scansindicating scan_type=fluo-ntfy.  These hints are passed to Accumulo tabletservers and are ignored by default. For these scan types, Accumulo could beconfigured to either send them to a special thread pool and/or prioritize themdifferently within a thread pool.  Below is an example of Accumulo shellcommands that set up a special executor for notification scans.config -s tserver.scan.executors.fnotify.threads=1config -t fluo_table -s table.scan.dispatcher=org.apache.accumulo.core.spi.scan.SimpleScanDispatcherconfig -t fluo_table -s table.scan.dispatcher.opts.executor.fluo-ntfy=fnotifyThe system setting tserver.scan.executors.fnotify.threads=1 creates a single-threadedscan executor in each tablet server named fnotify. The two per-tablesettings configure a scan dispatcher (the SimpleScanDispatcher is built intoAccumulo) on the table fluo_table.  The scan dispatcher is configured such that whena scan hint of scan_type=fluo-ntfy is seen, it runs on the executor fnotify.All other scans will run on the default executor. This has the effect runningall notification scans on a single dedicated thread in each tablet server.The above setting were tested in a scenario where 20 Fluo worker were runagainst a single tablet server with 20 tablets.  The Fluo stress test was runwith a low ingest rate, resulting in continual notification scanning by the 20workers.  While the test was running, jstack and top were used to inspect thetablet server. This inspection revealed that notification scans were allrunning in a single thread which was using 100% of a single core.  This left allof the other cores free to process transactions.  Further testing to see howthis impacts throughput is needed. Observing the worker debug logs, all of themseemed to complete notification scans, quickly finding new work.Fluo took a descriptive approach to using scan hints, where it described to Accumulothe type of scan it was running.  However, Fluo does not care what, ifanything, Accumulo does with that information.  This allows administrators toconfigure Accumulo in many different ways to handle notification scans, withoutany changes to Fluo.For my first pass at using scan executors, I tried a prescriptive approach. Iattempted to use scan hints to explicitly name an executor for notificationscans.  I realized this would require Fluo configuration to provide the name ofthe scan executor. Forcing a user to specify Accumulo and Fluo configurationwas very cumbersome so I abandoned the prescriptive approach.  The descriptiveapproach I settled on in its place is less cumbersome (it only requires Accumuloconfig) and more flexible (it supports executors and/or prioritization insteadof only executors).At the time of this writing, no released version of Fluo supports Accumulo 2.0.Once Fluo 1.3.0 is released with Accumulo 2.0, Hadoop 3.0, and Java 11 support,it will include support for scan executors.",
      "url": " /blog/2019/09/30/scan-executors/",
      "categories": "blog"
    }
    ,
  
    "release-fluo-yarn-1-0-0": {
      "title": "Apache Fluo YARN 1.0.0",
      "content"	 : "This is the first release of the Apache Fluo YARN launcher which runs Fluo applications inHadoop YARN. The YARN launcher was originally distributed with Fluo. It has beenmoved to its own project as Fluo has multiple application launchers. Moving the code outsimplified Fluo and reduced its dependencies.This release of the Fluo YARN launcher requires Fluo 1.2.0 or above. Future releases of Fluo shouldwork with this release as the YARN launcher uses minimal Fluo API. If not, a new release will be made. However,new releases of the YARN launcher will likely be due to new releases of Hadoop YARN and Apache Twill.Below are resources for this release:      Download a release tarball and verify by these procedures using these KEYS                            fluo-yarn-1.0.0-bin.tar.gz          ASC SHA                          fluo-yarn-1.0.0-source-release.tar.gz          ASC SHA                          View documentation for to learn how to run Fluo application in YARN.  TestingWhile testing Fluo 1.2.0, two stress test runs used an unreleased version of Fluo YARN.",
      "url": " /release/fluo-yarn-1.0.0/",
      "categories": "release"
    }
    ,
  
    "release-fluo-recipes-1-2-0": {
      "title": "Apache Fluo Recipes 1.2.0",
      "content"	 : "Apache Fluo Recipes builds on the Apache Fluo API to provide libraries of common code for Fluo developers.The 1.2.0 release is the first release of Fluo Recipes since Fluo graduated from Apache incubation. It has veryfew changes from the 1.1.0-incubating release. If you are new to Fluo Recipes, you should use this release. If youare already using Fluo Recipes 1.1.0-incubating, there is no reason to upgrade immediately.Below are resources for this release:  Release artifacts have been pushed to Maven Central. They can be used by updating your pom.xml:    &amp;lt;dependency&amp;gt;  &amp;lt;groupId&amp;gt;org.apache.fluo&amp;lt;/groupId&amp;gt;  &amp;lt;artifactId&amp;gt;fluo-recipes-core&amp;lt;/artifactId&amp;gt;  &amp;lt;version&amp;gt;1.2.0&amp;lt;/version&amp;gt;&amp;lt;/dependency&amp;gt;        A source release tarball is available. It can be verified by these procedures using these KEYS          fluo-recipes-1.2.0-source-release.tar.gz - ASC SHA        View the documentation for this release  Read the javadocs: core, accumulo, kryo, spark, test  View all changes.Notable ChangesDocumentation moved to project websiteThe documentation for Fluo Recipes now lives on the project website. In #144, it was removed from the Fluo Recipes repoand moved to Fluo Website repo.Updated versionsFluo Recipes was updated (in #146) to build using Fluo 1.2.0 and Accumulo 1.7.3",
      "url": " /release/fluo-recipes-1.2.0/",
      "categories": "release"
    }
    ,
  
    "release-fluo-1-2-0": {
      "title": "Apache Fluo 1.2.0",
      "content"	 : "Apache Fluo is a distributed processing system that lets users make incrementalupdates to large data sets.  With Apache Fluo, users can set up workflows thatexecute cross node transactions when data changes. These workflows enable usersto continuously join new data into large existing data sets withoutreprocessing all data.  Apache Fluo is built on Apache Accumulo.Below are resources for this release:      Download a release tarball and verify by these procedures using these KEYS                            fluo-1.2.0-bin.tar.gz          ASC SHA                          fluo-1.2.0-source-release.tar.gz          ASC SHA                      View the documentation for this release.  Read the Javadocs.Apache Fluo follows semver for its API . The API consistsof everything under the org.apache.fluo.api package. Code outside of thispackage can change at any time. If your project is using Fluo code that fallsoutside of the API, then consider initiating a discussionabout adding it to the API.Notable changesThe major changes in 1.2.0 are highlighted here, for the complete list of changes, see the 1.2.0Milestone on Github.Multiple ways of running applicationsBefore 1.2.0, Fluo applications could only be run in YARN or as local processes. For 1.2.0, the Fluo tarballwas refactored to support other ways of running Fluo applications such as Docker, Kubernetes &amp;amp; Marathon. Thisrefactoring has the following notable changes:  The Fluo tarball is now designed for only initializing Fluo and running local processes.  The fluo.properties configuration file was deprecated and was replaced by fluo-conn.properties and fluo-app.properties.          fluo-conn.properties contains connection properties and is shared by Fluo applications      fluo-app.properites contains application-specific properties and is only used during initialization of Fluo applications.        Code for launching Fluo applications in YARN was moved to its own project that has its ownrelease tarball. The Fluo tarball can still be used to launch Fluo applicationsin YARN if a fluo.properties is created from fluo.properties.deprecated.  Fluo application jars are now stored in HDFS and application configuration is now stored in Zookeeper.  This information used to be stored in a local directory.  This change made running Fluo different ways (like YARN or Docker) possible.  The fluo get-jars command (added in db0bdad) can be used to retrieve application jars.  Fluo configuration options can now be specified on the command line with -o option.  This enables passing Fluo options on the Docker command line.  Fluo scripts now support passing Java options.  In Docker, its useful to have control over the amount of memory Java uses.  This can be done by setting the FLUO_JAVA_OPTS env variable on the Docker command line.  This was added in commit 4207df4.Read the quickstart documentation to learn how to run Fluo applications using these new methods.Read locks.The Percolator paper stated that read locks were expensive and usually notneeded.  Therefore in Percolator reads did not acquire a read lock.  Thisassessment is correct, not every read should acquire a read lock.  However,offering the ability to optionally obtain a read lock makes writing certainapplications much simpler.  So in this release of Fluo, optional read lockswere added. Below is an example of how to acquire read locks.  void addEdge(FluoClient client, String node1, String node2) {    try(Transaction tx = client.newTransaction()) {      // These reads acquire a read lock.  Any concurrent changes will cause this      // transaction to fail.      String a1 = tx.withReadLock().gets(node1, new Column(&quot;node&quot;,&quot;alias&quot;));      String a2 = tx.withReadLock().gets(node2, new Column(&quot;node&quot;,&quot;alias&quot;));      tx.set(&quot;e:&quot;+a1+&quot;:&quot;+a2, new Column(&quot;edge&quot;, &quot;exists&quot;), &quot;Y&quot;);    }  }  void setAlias(FluoClient client, String node, String newAlias) {    try(Transaction tx = client.newTransaction()) {      String oldAlias = tx.gets(node, new Column(&quot;node&quot;,&quot;alias&quot;));      tx.set(node, new Column(&quot;node&quot;,&quot;alias&quot;), newAlias);      updateExistingEdges(oldAlias, newAlias);    }  }Concurrent calls to addEdge(client,&quot;n1&quot;,&quot;n2&quot;) and addEdge(client,&quot;n1&quot;,&quot;n3&quot;)can run without issue.  However, concurrent calls toaddEdge(client,&quot;n1&quot;,&quot;n2&quot;) and setAlias(client, &quot;n1&quot;,&quot;a5&quot;) will result in acollision.  If addEdge did not obtain a read lock, then it would not collidewith setAlias.  If addEdge obtained a write lock, then concurrent calls toaddEdge could needlessly collide.See the withReadLock javadoc for more information.Simplified Scanner APIRange scanning API was simplified in c737df6.  Before, scanning over a row was :CellScanner scanner = snapshot.scanner().over(Span.exact(&quot;row&quot;)).build();After this change, it can be written as :CellScanner scanner = snapshot.scanner().over(&quot;row&quot;).build();StreamsIn 2f11089 support for Java 8 streams was added.  The followingcomputes unique columns in rows with the prefix abc.Set&amp;lt;Column&amp;gt; cols = snapshot.scanner().overPrefix(&quot;abc&quot;).build()    .stream().map(RowColumnValue::getColumn).collect(Collectors.toSet());Remove command.Can now remove applications data in Zookeeper andAccumulo. Work on this issue was done in #999.  Beforedata had to be manually removed from Zookeeper and Accumulo.FluoConfiguration JavadocMost of the methods in FluoConfiguration get or a set a specific property.Before it was not easy to know which property. In cbe44b1 thejavadoc was updated to mention the properties.Shaded libthriftFluo uses Apache Thrift for remote procedure calls.  Projects using Thrift useits compiler to generate code.  This generated Java code make calls tolibthrift which is an artifact release by the Thrift project.  The codegenerated by a specific version of Thrift is only guaranteed to work with thesame version of libthrift.  For example, code generated by the Thrift 0.9.1compiler is only guaranteed to work with libthrift 0.9.1.Accumulo also uses Thrift for its RPCs.  When Accumulo and Fluo use differentversions of thrift it can cause serious problems. To avoid these problems,in 082ee8e libthrift was shaded and relocated into the fluo-core jar eliminating Fluo’sexternal dependency on libthrift.  This means that no matter which versionAccumulo uses, it will not conflict with Fluo’s version.Asynchronous commit refactoredFluo’s commit code is asynchronous in order to support high throughput.Before this release the high level commit logic was spread far and wide in thecode.  For this release the commit code was transitioned from Guava’sListenableFuture to Java 8’s CompletableFuture in ca63aaf.  This transition laidthe ground work for 6bf604f which centralized the commit logic.  Now the highlevel logic for the commit code is all in one place, making it much easier tounderstand.Other notable changes  7c16599 Added a method for efficiently comparing Byte and byte[]  151c565 Added fluo status command.  #960 Fluo started using some Accumulo APIs introduced in 1.7.0.  Therefore 1.7.0 is now the minimum version of Accumulo.  A few of Fluo’s internal caches were made configurable in 650f044, 9e87f47, and 955c86f.  Optimized Fluo’s server side code that runs in Accumulo tablet servers in 202fe08 and 51dc912  Added method to merge Fluo configuration objects in a349a9d  Initialization from Java API and command line now behave the same. Before 1.2, only command line initialization copied Fluo’s server side code to HDFS.  Now the Java API also does this and it controlled by configuration.TestingThe Fluo stress test was run twice as documented here and here.",
      "url": " /release/fluo-1.2.0/",
      "categories": "release"
    }
    ,
  
    "blog-2017-07-26-fluo-graduates-from-apache-incubator": {
      "title": "Fluo graduates from the Apache incubator!",
      "content"	 : "Apache Fluo graduated from the Apache Incubator to become a Top-Level Project!Graduation signifies Fluo’s commitment to the meritocratic process and principles of the ApacheSoftware Foundation. For more information, see the following press announcements :  Apache Software Foundation blog post  @TheASF Tweet  NASDAQ GlobeNewswireFor general information about Fluo, checkout the home page.",
      "url": " /blog/2017/07/26/fluo-graduates-from-apache-incubator/",
      "categories": "blog"
    }
    ,
  
    "release-fluo-recipes-1-1-0-incubating": {
      "title": "Apache Fluo Recipes 1.1.0-incubating",
      "content"	 : "Apache Fluo Recipes builds on the Apache Fluo API to provide libraries of common code for Fluo developers.Below are resources for this release:      Download a release tarball and verify by these procedures using these KEYS                            fluo-recipes-1.1.0-incubating-source-release.tar.gz          ASC MD5 SHA                      View the documentation  Read the javadocs: core, accumulo, kryo, spark, test  Jars are available in Maven Central.  View the changes.Major ChangeFor this release of Fluo Recipes, the work done in #127, #128, #130, and #131 to support the new Observer API wasthe most significant change.  The Collision Free Map and Export Queue requiredsignificant additions to support the new Observer API.  Since the nameCollision Free Map (CFM) is awful and it needed major API additions, thedecision was made to deprecate it and offer the CombineQueue.  TheCombineQueue offers the  same functionality as the CFM, but only supports thenew observer API. The deprecated CFM still supports the old Observer API.  Forthe Export Queue, additions were made to its API and everything related to theold Observer API was deprecated.  All API changes in this release are backwardscompatible with the 1.0.0 release.Example of new APIsThe new APIs in this release are much easier to use and now offer the abilityto use lambdas.  This example attempts to shows this and does the following :  Counts events in three dimensions (x,y,t).  Counts events in the two dimensional cross sections : (x,y), (x,t), and (y,t).  Prints out the counts as they change.To illustrate what this example accomplishes, for the following inputs :  2 events at (x=3,y=3,t=5)  1 events at (x=3,y=3,t=5)  4 events at (x=7,y=3,t=5)The example code should compute the following.  3 events at (x=3,y=3,t=5)  4 events at (x=7,y=3,t=5)  3 events at (x=3,y=3)  4 events at (x=7,y=3)  3 events at (x=3,t=5)  4 events at (x=7,t=5)  7 events at (y=3,t=5)The example achieves this using recipes as follows :  An export queue that prints out all changes in counts.  Three combine queues for counting 2D cross sections.  All three queue data for export when counts change.  A combine queue for counting 3D events.  It queues updates to the 2D combine queues when counts changes.  It also queues changes to the export queue.Below is the Fluo ObserverProvider that wires everything together. The newFluo and Fluo Recipes APIs enable wiring everything in Java code.  In theprevious versions, this would have been a cumbersome combination ofconfiguration and Java code.   With the new APIs, using lambdas is now anoption.  This was not an option with the old APIs.public class AppObserverProvider implements ObserverProvider {  @Override  public void provide(Registry obsRegistry, Context ctx) {    SimpleConfiguration appCfg = ctx.getAppConfiguration();    CombineQueue&amp;lt;String, Long&amp;gt; xytCq = CombineQueue.getInstance(Example.CQ_XYT_ID, appCfg);    CombineQueue&amp;lt;String, Long&amp;gt; xyCq = CombineQueue.getInstance(Example.CQ_XY_ID, appCfg);    CombineQueue&amp;lt;String, Long&amp;gt; ytCq = CombineQueue.getInstance(Example.CQ_YT_ID, appCfg);    CombineQueue&amp;lt;String, Long&amp;gt; xtCq = CombineQueue.getInstance(Example.CQ_XT_ID, appCfg);    ExportQueue&amp;lt;String, String&amp;gt; exportQ = ExportQueue.getInstance(Example.EXPORTQ_ID, appCfg);    // Some of Lambda&#39;s below could be inlined. To make the example a little more clear they were    // not in order to show the types involved.    // This is called by a combine queue when a value changes. The old and new value for the key    // are passed. The lambda below queues changes for export.    ChangeObserver&amp;lt;String, Long&amp;gt; expChangeObs = (tx, changes) -&amp;gt; {      for (Change&amp;lt;String, Long&amp;gt; change : changes) {        String oldVal = change.getOldValue().map(v -&amp;gt; &quot;old: &quot; + v).orElse(&quot;old: -&quot;);        String newVal = change.getNewValue().map(v -&amp;gt; &quot;new: &quot; + v).orElse(&quot;new: -&quot;);        exportQ.add(tx, change.getKey(), oldVal + &quot; &quot; + newVal);      }    };    // This lambda processes changes to 3D counts. It queues updates to the (x,y), (x,t), and (y,t)    // 2D combine queues. For example if (x=3,y=2,t=5) changed from 4 to 7, it would queue    // (x=3,y=2):+3, (x=3,t=5):+3, and (y=2,t=5):+3 to the 2D combine queues. The lambda also queues    // exports for 3D count changes.    ChangeObserver&amp;lt;String, Long&amp;gt; projectingChangeObs = (tx, changes) -&amp;gt; {      Map&amp;lt;String, Long&amp;gt; xtUpdates = new HashMap&amp;lt;&amp;gt;();      Map&amp;lt;String, Long&amp;gt; ytUpdates = new HashMap&amp;lt;&amp;gt;();      Map&amp;lt;String, Long&amp;gt; xyUpdates = new HashMap&amp;lt;&amp;gt;();      for (Change&amp;lt;String, Long&amp;gt; change : changes) {        String[] fields = change.getKey().split(&quot;:&quot;);        long delta = change.getNewValue().orElse(0L) - change.getOldValue().orElse(0L);        // While processing the changes for an entire bucket, opportunistically merge multiple        // updates to the same 2D coordinates.        xtUpdates.merge(fields[0] + &quot;:&quot; + fields[2], delta, Long::sum);        ytUpdates.merge(fields[1] + &quot;:&quot; + fields[2], delta, Long::sum);        xyUpdates.merge(fields[0] + &quot;:&quot; + fields[1], delta, Long::sum);      }      // Queue updates to 2D combine queues.      xtCq.addAll(tx, xtUpdates);      ytCq.addAll(tx, ytUpdates);      xyCq.addAll(tx, xyUpdates);      // Queue changes for export      expChangeObs.process(tx, changes);    };    // Register observer for 3D combine queue. The observer calls the provided combiner and    // change observer when processing queued entries.    xytCq.registerObserver(obsRegistry, new SummingCombiner&amp;lt;&amp;gt;(), projectingChangeObs);    // Register observers for all 2D combine queues.    xyCq.registerObserver(obsRegistry, new SummingCombiner&amp;lt;&amp;gt;(), expChangeObs);    xtCq.registerObserver(obsRegistry, new SummingCombiner&amp;lt;&amp;gt;(), expChangeObs);    ytCq.registerObserver(obsRegistry, new SummingCombiner&amp;lt;&amp;gt;(), expChangeObs);    // This functional interface is new in this release. The lambda below prints out data that was    // successfully queued for export.    Exporter&amp;lt;String, String&amp;gt; exporter = iter -&amp;gt; {      while (iter.hasNext()) {        SequencedExport&amp;lt;String, String&amp;gt; seqExport = iter.next();        System.out.printf(&quot;EXPORT %-15s %-15s seq: %dn&quot;, seqExport.getKey(), seqExport.getValue(),            seqExport.getSequence());      }    };    // Register an observer to process queued export entries. The observer will call the lambda    // created above.    exportQ.registerObserver(obsRegistry, exporter);  }}The code below does three things :  Starts MiniFluo.  Configures the four combine queues and the export queue.  Adds some data to the 3D combine queue twice.  Between the adds, it waits for processing to finish.    FluoConfiguration props = new FluoConfiguration();    props.setApplicationName(&quot;dimensions&quot;);    props.setMiniDataDir(&quot;target/mini&quot;);    CombineQueue.configure(CQ_XYT_ID).keyType(String.class).valueType(Long.class).buckets(7).save(props);    CombineQueue.configure(CQ_XT_ID).keyType(String.class).valueType(Long.class).buckets(7).save(props);    CombineQueue.configure(CQ_XY_ID).keyType(String.class).valueType(Long.class).buckets(7).save(props);    CombineQueue.configure(CQ_YT_ID).keyType(String.class).valueType(Long.class).buckets(7).save(props);    // A new Fluent method of configuring export queues was introduced in 1.1.0    ExportQueue.configure(EXPORTQ_ID).keyType(String.class).valueType(String.class).buckets(7).save(props);    props.setObserverProvider(AppObserverProvider.class);    FileUtils.deleteQuietly(new File(&quot;target/mini&quot;));    try (MiniFluo miniFluo = FluoFactory.newMiniFluo(props);          FluoClient fc = FluoFactory.newClient(miniFluo.getClientConfiguration())) {      CombineQueue&amp;lt;String,Long&amp;gt; xytCq = CombineQueue.getInstance(CQ_XYT_ID, fc.getAppConfiguration());      Map&amp;lt;String,Long&amp;gt; updates = new HashMap&amp;lt;&amp;gt;();      updates.put(&quot;x=3:y=5:t=23&quot;, 1L);      updates.put(&quot;x=5:y=5:t=27&quot;, 1L);      updates.put(&quot;x=3:y=5:t=27&quot;, 1L);      try (Transaction tx = fc.newTransaction()) {        xytCq.addAll(tx, updates);        tx.commit();      }      miniFluo.waitForObservers();      System.out.println(&quot;n*** All notifications processed. ***n&quot;);      updates.clear();      updates.put(&quot;x=3:y=5:t=23&quot;, 1L);      updates.put(&quot;x=5:y=5:t=27&quot;, -1L);      updates.put(&quot;x=3:y=5:t=29&quot;, 1L);      try (Transaction tx = fc.newTransaction()) {        xytCq.addAll(tx, updates);        tx.commit();      }      miniFluo.waitForObservers();      System.out.println(&quot;n*** All notifications processed. ***n&quot;);    }Below is the output of running this example.EXPORT x=3:y=5:t=23    old: - new: 1   seq: 8EXPORT x=3:y=5:t=27    old: - new: 1   seq: 9EXPORT x=5:y=5:t=27    old: - new: 1   seq: 9EXPORT x=3:y=5         old: - new: 2   seq: 37EXPORT y=5:t=27        old: - new: 2   seq: 42EXPORT x=3:t=23        old: - new: 1   seq: 36EXPORT x=5:t=27        old: - new: 1   seq: 36EXPORT x=3:t=27        old: - new: 1   seq: 38EXPORT x=5:y=5         old: - new: 1   seq: 39EXPORT y=5:t=23        old: - new: 1   seq: 41*** All notifications processed. ***EXPORT x=3:y=5:t=29    old: - new: 1   seq: 92EXPORT x=5:y=5:t=27    old: 1 new: -   seq: 92EXPORT x=3:y=5:t=23    old: 1 new: 2   seq: 93EXPORT y=5:t=27        old: 2 new: 1   seq: 109EXPORT x=3:y=5         old: 2 new: 4   seq: 110EXPORT y=5:t=23        old: 1 new: 2   seq: 111EXPORT y=5:t=29        old: - new: 1   seq: 108EXPORT x=3:t=29        old: - new: 1   seq: 105EXPORT x=3:t=23        old: 1 new: 2   seq: 106EXPORT x=5:y=5         old: 1 new: -   seq: 107EXPORT x=5:t=27        old: 1 new: -   seq: 106*** All notifications processed. ***",
      "url": " /release/fluo-recipes-1.1.0-incubating/",
      "categories": "release"
    }
    ,
  
    "release-fluo-1-1-0-incubating": {
      "title": "Apache Fluo 1.1.0-incubating",
      "content"	 : "Below are resources for this release:      Download a release tarball and verify by these procedures using these KEYS                            fluo-1.1.0-incubating-bin.tar.gz          ASC MD5 SHA                          fluo-1.1.0-incubating-source-release.tar.gz          ASC MD5 SHA                      View the documentation for this release.  Read the Javadocs.Apache Fluo follows semver for its API . The API consistsof everything under the org.apache.fluo.api package. Code outside of thispackage can change at any time. If your project is using Fluo code that fallsoutside of the API, then consider initiating a discussionabout adding it to the API.Significant changesThe major changes in 1.1.0 are highlighted here, for the complete list of changes, see the 1.1.0Milestone on Github.New API for configuring observers.The 1.0.0 API for providing Observers required configuring an Observer class for each observedcolumn.  This API was cumbersome to use and made using lambdas impossible.  For #816 a better APIwas introduced.   The new API only requires configuring a single class that provides all Observers.This single class can register lambdas to observe a column.  This new API makes writing Fluoapplications faster and easier.  Below is an example of using the new API to register two observersthat compute the number of URLs that reference a document.public class AppObserverProvider implements ObserverProvider {  private static final Column DOC_CURR_COL = new Column(&quot;doc&quot;, &quot;curr&quot;);  private static final Column DOC_NEW_COL = new Column(&quot;doc&quot;, &quot;new&quot;);  private static final Column DOC_URL_CHANGE = new Column(&quot;doc&quot;, &quot;urlChange&quot;);  private static final Column DOC_REF_COUNT_COL = new Column(&quot;doc&quot;, &quot;refCount&quot;);  // Each Fluo worker will call this method to create the observers it needs.  @Override  public void provide(Registry registry, Context ctx) {    // This could be used to pass application specific configuration to observers. Its not used in    // this example.    SimpleConfiguration appConfig = ctx.getAppConfiguration();    // Register an observer that processes new content for a document.    registry.forColumn(DOC_NEW_COL, STRONG).useObserver(new ContentObserver());    // Register a lambda that processes notifications for the column DOC_URL_CHANGE.    registry.forColumn(DOC_URL_CHANGE, WEAK).useStrObserver((tx, myUrl, col) -&amp;gt; {      // Compute the number of URLs that refer to this URL.      CellScanner refScanner = tx.scanner().over(Span.exact(myUrl, new Column(&quot;ref&quot;))).build();      int numRefs = Iterables.size(refScanner);      // Do something interesting with count.  This is not interesting, but keeps the example short.      tx.set(myUrl, DOC_REF_COUNT_COL, numRefs + &quot;&quot;);    });  }  /**   * Compute the change in a documents URLs and propagate those to other documents.   */  private static class ContentObserver implements StringObserver {    @Override    public void process(TransactionBase tx, String myUrl, Column col) throws Exception {      // Retrieve the new and current document content.      Map&amp;lt;Column, String&amp;gt; colVals = tx.gets(myUrl, DOC_CURR_COL, DOC_NEW_COL);      String newContent = colVals.getOrDefault(DOC_NEW_COL, &quot;&quot;);      String currContent = colVals.getOrDefault(DOC_CURR_COL, &quot;&quot;);      // Extract the URLs in the new and current document content.      Set&amp;lt;String&amp;gt; newUrls = extractUrls(newContent);      Set&amp;lt;String&amp;gt; currUrls = extractUrls(currContent);      // For URLs that only exist in new content, update the document they reference.      Sets.difference(newUrls, currUrls).forEach(url -&amp;gt; {        tx.set(url, new Column(&quot;ref&quot;, myUrl), &quot;&quot;);        tx.setWeakNotification(url, DOC_URL_CHANGE);      });      // For URLs that are no longer present, update the document they reference.      Sets.difference(currUrls, newUrls).forEach(url -&amp;gt; {        tx.delete(url, new Column(&quot;ref&quot;, myUrl));        tx.setWeakNotification(url, DOC_URL_CHANGE);      });      // Update the current document content.      tx.set(myUrl, DOC_CURR_COL, newContent);      tx.delete(myUrl, DOC_NEW_COL);    }    private Set&amp;lt;String&amp;gt; extractUrls(String newContent) {      // TODO implement extracting URLs from content      return null;    }  }}Before initializing a Fluo App, the ObserverProvider above would be added to configuration as follows.FluoConfiguration fluoConf = ...fluoConf.setObserverProvider(AppObserverProvider.class);// initialize Fluo app using fluoConfThe old API is still present but has been deprecated and may be removed in the future.Improved Fluo scalabilityIn the previous release each worker scanned the entire table looking for notifications that hashedto it.  This strategy for finding notifications is O(N*W) where  N is the number of notification andW is the number of workers.For #500 a different strategy was used to find notifications.  Workers divide themselves intogroups and each group scans a subset of the table for notifications.  Every worker in a group scansthe groups entire subset of a table  looking for notifications that hash to it. The new strategy resultsin O(N*G) work where N is the number of notifications and G is the group size.  By default the groupsize is 7, but this is configurable.  The group size may need to be increased if portion of a tableis popular and assigned to one group that can not processes it.To compare the old and new ways assume we have 109 notifications and 100 workers.  Theold method would have scanned 1011 entries to to find all notifications.  Assuming a groupsize of 7, the new strategy scans 7 * 109 entries to find all notifications.  Anice feature of the new strategy is that the amount of scanning is independent of the number of workers.For the old strategy if the number of workers increases by factor of 10, then the amount scanningwill increase by a factor of 10.  So for 1,000 workers the old strategy would scan1012 entries to find all notifications.  The new strategy will still only scan 7 *109 entries with 1,000 workers.Improved BytesFluo’s API has an immutable wrapper for a byte array.  Multiple improvements were made to this bytewrapper.  startsWith(Bytes) and endsWith(Bytes) methods were added in #823  A copyTo(byte[]) method was added for #827  Internal performance improvements were made in #826 and #799Improved Spark integrationFor #813 improvements were made that allow easy passing of FluoConfiguration objects to remote Sparkprocesses.TestingLong runs of Stresso and Webindex were successfully completed on EC2 using multiple nodes.",
      "url": " /release/fluo-1.1.0-incubating/",
      "categories": "release"
    }
    ,
  
    "blog-2017-01-10-accumulo-summit-17": {
      "title": "Fluo talks at Accumulo Summit 2016",
      "content"	 : "Two Apache Fluo talks were given at the 2016 Accumulo Summit on October 11, 2016.  Below are links tothe talks.  Tips For Writing Fluo Applications : Summit page, Slides, Video  Accumulo Indexing Strategies for Searching Semantic Networks : Summit page, Slides, Video",
      "url": " /blog/2017/01/10/accumulo-summit-17/",
      "categories": "blog"
    }
    ,
  
    "blog-2016-12-22-spark-load": {
      "title": "Loading data into Fluo using Apache Spark",
      "content"	 : "Apache Spark can be used to preprocess and load batches of data into Fluo.  For exampleSpark could be used to group data within a batch and then Fluo transactions could load groups ofrelated data. This blog post offers some tips to help you get started writing to Fluo from Spark.Executing load transactions in SparkSpark automatically serializes Java objects that are needed for remote execution.  When trying touse Fluo with Spark its important to understand what will serialize properly and what will not.Classes used to load data into Fluo like FluoClient and LoaderExecutor are not suitable forserialization.  These classes may have thread pools, resources in Zookeeper, transactions that arecommitting in the background, etc .  Therefore these classes must be instantiated at each remote processSpark creates.  One way to do this is with Spark’s foreachParition method.  This method willexecute code locally at each RDD partition. Within each partition, a LoaderExecutorcan be created.  That’s what the example below shows. public void dedupeAndLoad(JavaRDD&amp;lt;Document&amp;gt; docRdd, int numPartitions) {    // Remove duplicate documents.  docRdd = docRdd.distinct(numPartitions);    // Execute load transactions for unique documents.  Iin Java 8 lambda syntax below,   // iter is of type Iterator&amp;lt;Document&amp;gt;  docRdd.foreachPartition(iter-&amp;gt;{    // Assume fluo.properties file was submitted with application    FluoConfiguration fconf = new FluoConfiguration(new File(&quot;fluo.properties&quot;));    try(FluoClient client = FluoFactory.newClient(fconf);         LoaderExecutor le = client.newLoaderExecutor())    {      while(iter.hasNext()) {        le.execute(new DocumentLoader(iter.next()));      }    }  });}The example above requires that fluo.properties is available locally for eachpartition.  This can be accomplished with --files option when launching a Spark job.spark-submit --class myApp.Load --files &amp;lt;fluo props dir&amp;gt;/fluo.properties myApp.jarIf FluoConfiguration were serializable, then Spark could automatically serialize and make aFluoConfiguration object available for each partition.  However, FluoConfiguration is notserializable as of Fluo 1.0.0.  This will be fixed in future releases of Fluo.  See #813for details and workarounds for 1.0.0.Initializing Fluo tableIf you have a lot of existing data, then you could use Spark to initialize your Fluo table withhistorical data. There are two general ways to do this.  The simplest way is to use theAccumuloOutputFormat to write Mutation objects to Accumulo.  However, you need to write datausing the Fluo data format.  Fluo provides an easy way to do this using the FluoMutationGenerator.A slightly more complex way to initialize a Fluo table is using Accumulo’s bulk load mechanism.Bulk load is the process of generating Accumulo RFile’s containing Key/Values in a Spark job. Thosefiles are then loaded into an Accumulo table.   This can be faster, but its more complex because itrequires the user to properly partition data in their Spark job.  Ideally, these partitions wouldconsist of non-overlapping ranges of Accumulo keys with roughly even amounts of data.  The defaultpartitioning methods in Spark will not accomplish this.When following the bulk load approach, you would write Key and Value objects using theAccumuloFileOutputFormat. Fluo provides the FluoKeyValueGenerator to create key/values in theFluo data format.  Fluo Recipes builds on this and provides code that makes it easy to bulk importinto Accumulo.  The FluoSparkHelper.bulkImportRcvToFluo() method will do the following :  Repartition data using the split points in the Fluo table  Convert data into expected format for a Fluo table  Create an RFile for each partition in a specified temp dir  Bulk import the RFiles into the Fluo tableThe Webindex example uses bulk load to initialize its Fluo table using the code in Fluo Recipes.Webindex uses multiple Collision Free Maps and initializes them usingCollisionFreeMap.getInitializer().  Webindex uses Spark to initialize the Fluo table withhistorical data.  Webindex also uses Spark to execute load transactions in parallel forincrementally loading data.Packaging your code to run in SparkOne simple way to execute your Spark code is to create a shaded jar.  This shaded jar should contain: Accumulo client code, Fluo client code, Zookeeper client code, and your Application code.  Itwould be best if the shaded jar contained the versions of Accumulo, Fluo, and Zookeeper running onthe target system.  One way to achieve this goal is to make it easy for users of your Fluoapplication to build the shaded jar themselves.  The examples below shows a simple bash script andMaven pom file that achieve this goal.There is no need to include Spark code in the shaded jar as this will be provided by the Sparkruntime environment.   Depending on your Spark environment, Hadoop client code may also be provided.Therefore, Hadoop may not need to be included in the shaded jar. One way to exclude these from theshaded jars is to make the scope of these dependencies provided, which is what the example does.You may also want to consider excluding other libraries that are provided in the Spark env likeGuava, log4j, etc.&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&amp;gt;&amp;lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&amp;gt;  &amp;lt;modelVersion&amp;gt;4.0.0&amp;lt;/modelVersion&amp;gt;  &amp;lt;groupId&amp;gt;com.foo&amp;lt;/groupId&amp;gt;  &amp;lt;artifactId&amp;gt;fluoAppShaded&amp;lt;/artifactId&amp;gt;  &amp;lt;version&amp;gt;0.0.1-SNAPSHOT&amp;lt;/version&amp;gt;  &amp;lt;packaging&amp;gt;jar&amp;lt;/packaging&amp;gt;  &amp;lt;name&amp;gt;Shaded Fluo App&amp;lt;/name&amp;gt;  &amp;lt;properties&amp;gt;    &amp;lt;accumulo.version&amp;gt;1.7.2&amp;lt;/accumulo.version&amp;gt;    &amp;lt;fluo.version&amp;gt;1.0.0-incubating&amp;lt;/fluo.version&amp;gt;    &amp;lt;zookeeper.version&amp;gt;3.4.9&amp;lt;/zookeeper.version&amp;gt;  &amp;lt;/properties&amp;gt;  &amp;lt;build&amp;gt;    &amp;lt;plugins&amp;gt;      &amp;lt;plugin&amp;gt;        &amp;lt;groupId&amp;gt;org.apache.maven.plugins&amp;lt;/groupId&amp;gt;        &amp;lt;artifactId&amp;gt;maven-shade-plugin&amp;lt;/artifactId&amp;gt;        &amp;lt;executions&amp;gt;          &amp;lt;execution&amp;gt;            &amp;lt;goals&amp;gt;              &amp;lt;goal&amp;gt;shade&amp;lt;/goal&amp;gt;            &amp;lt;/goals&amp;gt;            &amp;lt;phase&amp;gt;package&amp;lt;/phase&amp;gt;            &amp;lt;configuration&amp;gt;              &amp;lt;shadedArtifactAttached&amp;gt;true&amp;lt;/shadedArtifactAttached&amp;gt;              &amp;lt;shadedClassifierName&amp;gt;shaded&amp;lt;/shadedClassifierName&amp;gt;              &amp;lt;filters&amp;gt;                &amp;lt;filter&amp;gt;                  &amp;lt;artifact&amp;gt;*:*&amp;lt;/artifact&amp;gt;                  &amp;lt;excludes&amp;gt;                    &amp;lt;exclude&amp;gt;META-INF/*.SF&amp;lt;/exclude&amp;gt;                    &amp;lt;exclude&amp;gt;META-INF/*.DSA&amp;lt;/exclude&amp;gt;                    &amp;lt;exclude&amp;gt;META-INF/*.RSA&amp;lt;/exclude&amp;gt;                  &amp;lt;/excludes&amp;gt;                &amp;lt;/filter&amp;gt;              &amp;lt;/filters&amp;gt;            &amp;lt;/configuration&amp;gt;          &amp;lt;/execution&amp;gt;        &amp;lt;/executions&amp;gt;      &amp;lt;/plugin&amp;gt;    &amp;lt;/plugins&amp;gt;  &amp;lt;/build&amp;gt;  &amp;lt;!--       The provided scope is used for dependencies that should not end up in       the shaded jar.  The shaded jar is used to run Spark jobs. The Spark        launcher will provided Spark and Hadoop dependencies, so they are not       needed in the shaded jar.  --&amp;gt;  &amp;lt;dependencies&amp;gt;    &amp;lt;!-- The dependency on your Fluo application code.  Version of your app could be made configurable. --&amp;gt;    &amp;lt;dependency&amp;gt;      &amp;lt;groupId&amp;gt;com.foo&amp;lt;/groupId&amp;gt;      &amp;lt;artifactId&amp;gt;fluoApp&amp;lt;/artifactId&amp;gt;      &amp;lt;version&amp;gt;1.2.3&amp;lt;/version&amp;gt;    &amp;lt;/dependency&amp;gt;    &amp;lt;dependency&amp;gt;      &amp;lt;groupId&amp;gt;org.apache.fluo&amp;lt;/groupId&amp;gt;      &amp;lt;artifactId&amp;gt;fluo-api&amp;lt;/artifactId&amp;gt;      &amp;lt;version&amp;gt;${fluo.version}&amp;lt;/version&amp;gt;    &amp;lt;/dependency&amp;gt;    &amp;lt;dependency&amp;gt;      &amp;lt;groupId&amp;gt;org.apache.fluo&amp;lt;/groupId&amp;gt;      &amp;lt;artifactId&amp;gt;fluo-core&amp;lt;/artifactId&amp;gt;      &amp;lt;version&amp;gt;${fluo.version}&amp;lt;/version&amp;gt;    &amp;lt;/dependency&amp;gt;    &amp;lt;dependency&amp;gt;      &amp;lt;groupId&amp;gt;org.apache.accumulo&amp;lt;/groupId&amp;gt;      &amp;lt;artifactId&amp;gt;accumulo-core&amp;lt;/artifactId&amp;gt;      &amp;lt;version&amp;gt;${accumulo.version}&amp;lt;/version&amp;gt;    &amp;lt;/dependency&amp;gt;    &amp;lt;dependency&amp;gt;      &amp;lt;groupId&amp;gt;org.apache.zookeeper&amp;lt;/groupId&amp;gt;      &amp;lt;artifactId&amp;gt;zookeeper&amp;lt;/artifactId&amp;gt;      &amp;lt;version&amp;gt;${zookeeper.version}&amp;lt;/version&amp;gt;    &amp;lt;/dependency&amp;gt;    &amp;lt;dependency&amp;gt;      &amp;lt;groupId&amp;gt;org.apache.hadoop&amp;lt;/groupId&amp;gt;      &amp;lt;artifactId&amp;gt;hadoop-client&amp;lt;/artifactId&amp;gt;      &amp;lt;version&amp;gt;2.7.2&amp;lt;/version&amp;gt;      &amp;lt;scope&amp;gt;provided&amp;lt;/scope&amp;gt;    &amp;lt;/dependency&amp;gt;    &amp;lt;dependency&amp;gt;      &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;      &amp;lt;artifactId&amp;gt;spark-core_2.10&amp;lt;/artifactId&amp;gt;      &amp;lt;version&amp;gt;1.6.2&amp;lt;/version&amp;gt;      &amp;lt;scope&amp;gt;provided&amp;lt;/scope&amp;gt;    &amp;lt;/dependency&amp;gt;  &amp;lt;/dependencies&amp;gt;&amp;lt;/project&amp;gt;The following bash script can use the pom above to build a shaded jar.# Get the versions of Accumulo and Fluo running on the system.  Could let the# user of your Fluo application configure this and have this script read that# config.ACCUMULO_VERSION=`accumulo version`FLUO_VERSION=`fluo version`# Could not find an easy way to get zookeeper version automaticallyZOOKEEPER_SERVER=localhostZOOKEEPER_VERSION=`echo status | nc $ZOOKEEPER_SERVER 2181 | grep version: | sed &#39;s/.*version: ([0-9.]*).*/1/&#39;`# Build the shaded jarmvn package -Daccumulo.version=$ACCUMULO_VERSION             -Dfluo.version=$FLUO_VERSION             -Dzookeeper.version=$ZOOKEEPER_VERSIONThere are other possible ways to package and run your Fluo application for Spark.  This sectionsuggested one possible way.  The core concept of this method is late binding of the Accumulo, Fluo,Hadoop, Spark, and Zookeeper libraries.  When choosing a method to create a shaded jar, theimplications of early vs late binding is something to consider.",
      "url": " /blog/2016/12/22/spark-load/",
      "categories": "blog"
    }
    ,
  
    "blog-2016-11-10-immutable-bytes": {
      "title": "Java needs an immutable byte string",
      "content"	 : "Fluo Data Model and TransactionsFluo uses a data model composed of key/values.  Each key has four fields :row,family,qualifier,visibility.  Each of these key fields is a sequence of bytes.  Fluotransactions read key/values from a snapshot of a table.  Any changes a transaction makes isbuffered until commit.  At the time of commit the changes are only made if no other transactionmodified any of the key values.While designing the Fluo API we were uncertain about making Fluo’s basic POJOs mutable orimmutable.  In the end we decided to go with immutable types to make writing correct Fluo codeeasier.  One of the POJOs we created was Bytes,  an immutable wrapper around a byte array.  Wealso created BytesBuilder, which is analogous to StringBuilder, and makes it easy and efficientto construct Bytes.What about the copies?Bytes requires a defensive copy at creation time.  When we were designing Fluo’s API we were worriedabout this at first.  However a simple truth became apparent.  If the API took a mutable type, thenall boundary points between the user and Fluo would require defensive copies.  For example assumeFluo’s API took byte arrays and consider the following code.//A Fluo transactionTransaction tx = ...byte[] row = ...tx.set(row, column1, value1);tx.set(row, column2, value2);tx.set(row, column3, value3);Fluo will buffer changes until a transaction is committed.  In the example above since Fluo acceptsa mutable row, it would be prudent to do a defensive copy each time set() is called above.In the code below where an immutable byte array wrapper is used, the calls to set() do not need todo a defensive copy.  So when comparing the two examples, the immutable byte wrapper results in lessdefensive copies.//A Fluo transactionTransaction tx = ...Bytes row = ...tx.set(row, column1, value1);tx.set(row, column2, value2);tx.set(row, column3, value3);We really did not want to create Bytes and BytesBuilder types, however we could not find what weneeded in Java’s standard libraries.  The following sections discuss some of the options weconsidered.Why not use String?Java’s String type is an immutable wrapper around a char array.  In order to store a byte array in aString, Java must decode the byte array using a character set.  Some sequences of bytes do not mapin some characters sets.  Therefore, trying to stuff arbitrary binary data in a String can corruptthe data.  The following little program shows this, it will print false.    byte bytes1[] = new byte[256];    for(int i = 0; i&amp;lt;255; i++)      bytes1[i] = (byte)i;    byte bytes2[] = new String(bytes1).getBytes();    System.out.println(Arrays.equals(bytes1, bytes2));String can be made to work by specifying an appropriate character set. The following program willprint true.  However, this is error prone and inefficient.  It’s error prone in the case where thecharacter set is wrong or omitted.  It’s inefficient because it results in copying from byte arraysto char arrays and visa versa.  Also, char arrays use twice as much memory.    byte bytes1[] = new byte[256];    for(int i = 0; i&amp;lt;255; i++)      bytes1[i] = (byte)i;    String str = new String(bytes1, StandardCharsets.ISO_8859_1);    byte bytes2[] = str.getBytes(StandardCharsets.ISO_8859_1);    System.out.println(Arrays.equals(bytes1, bytes2));Why not use ByteBuffer?A read only ByteBuffer might seem like it would fit the bill of an immutable byte array wrapper.However, the following program shows two ways that ByteBuffer falls short.  ByteBuffers are greatfor I/O, but it would not be prudent to use them when immutability is desired.    byte[] bytes1 = new byte[] {1,2,3,(byte)250};    ByteBuffer bb1 = ByteBuffer.wrap(bytes1).asReadOnlyBuffer();    System.out.println(bb1.hashCode());    bytes1[2]=89;    System.out.println(bb1.hashCode());    bb1.get();    System.out.println(bb1.hashCode());The program above prints the following, which is less than ideal :74772183036726786This little program shows two things.  First, the only guarantee we are getting fromasReadOnlyBuffer() is that bb1 can not be used to modify bytes1.  However, the originator ofthe read only buffer can still modify the wrapped byte array.   Java’s String and Fluo’s Bytes avoidthis by always copying data into an internal private array that never escapes.The second issue is that bb1 has a position and calling bb1.get() changes this position.Changing the position conceptually changes the contents of the ByteBuffer.  This is why hashCode()returns something different after bb1.get() is called.  So even though bb1 does not enablemutating bytes1, bb1 is itself mutable.Why not use Protobuf’s ByteString?Protocol Buffers has a beautiful implementation of an immutable byte array wrapper calledByteString.  I would encourage its use when possible.  I discovered this jewel after Bytes wasimplemented in Fluo.  Using it was considered, however in Fluo’s case its not really appropriate touse for two reasons.  First, any library designer should try to minimize what transitive dependenciesthey force on users.  Internally Fluo does not currently use Protocol Buffers in its implementation,so this would be a new dependency for Fluo users.  The second reason is going to require somebackground to explain.Technologies like OSGI and Jigsaw seek to modularize Java libraries and provide dependencyisolation.  Dependency isolation allows a user to use a library without having to share a librariesdependencies.  For example, consider the following hypothetical scenario.  Fluo’s implementation uses Protobuf version 2.5  Fluo user code uses Protobuf version 1.8Without dependency isolation, the user must converge dependencies and make their application andFluo use the same version of Protobuf.  Sometimes this works without issue, but sometimes thingswill break because Protobuf dropped, changed, or added a method.With dependency isolation, Fluo’s implementation and Fluo user code can easily use different versionsof Protobuf.  This is only true as long as Fluo’s API does not use Protobuf.  So, this is the secondreason that Fluo should not use classes from Protobuf in its API.  If Fluo used Protobuf in its APIthen it forces the user to have to converge dependencies, even if they are using OSGI or Jigsaw.Java should have an immutable byte array wrapperSo far, the following arguments have been presented:  An immutable byte array wrapper is useful and needed.  Java does not provide a good immutable byte array wrapper.  Using an immutable byte array wrapper from library X in library Y’s API may be problematic.These arguments all point to the need for an immutable byte array wrapper to exist in Java. Thisneed could also be satisfied by a library outside of Java with some restrictions. Assume a newlibrary called Lib Immutable Byte Array Wrapper (libibaw) was created.  In order for libibaw to beused in other libraries APIs, it would need to promise the following.  No dependencies other than Java.  Backwards compatibility.The reason backwards compatibility is important is that it would make dependency convergence supereasy.  The following situation shows this.  Fluo uses libibaw 1.2 in its API  Fluo user code uses libibaw 1.1.If libibaw promises backward compatibility, then all the user needs to do is use version 1.2 oflibibaw.  With the promise of backwards compatibility, using version 1.2 will not break the userscode.Having a library would be nice, but having something in Java would minimize copies.  Outsideof Java there will inevitably be multiple implementations and going between them will require acopy.  For example if a user uses Fluo and Protobuf they may be forced to copy Fluo’s Bytes toProtobuf’s ByteString. If Protobuf and Fluo both used an immutable byte sequence type from Java, thiswould not be needed.",
      "url": " /blog/2016/11/10/immutable-bytes/",
      "categories": "blog"
    }
    ,
  
    "release-fluo-recipes-1-0-0-incubating": {
      "title": "Apache Fluo Recipes 1.0.0-incubating",
      "content"	 : "Apache Fluo Recipes builds on the Apache Fluo API to provide libraries of common code for Fluo developers.Apache Fluo Recipes 1.0.0-incubating is the first release of Fluo Recipes as an Apache project and the thirdrelease for the project.Below are resources for this release:      Download a release tarball and verify by these procedures using these KEYS                            fluo-recipes-1.0.0-incubating-source-release.tar.gz          ASC MD5 SHA1                      View the documentation  Read the javadocs: core, accumulo, kryo, spark, test  Jars are available in Maven Central.Changes of interest since last release  #112 - Avoid allocating collection in AccumuloExporter  #107 - Added standard way to setup per exporter configuration  #102 - Simplified Accumulo export queue recipe  #92 - Added dependency analysis plugin  #82 - Moved TypeLayer from Fluo API to Fluo Recipes  #76 - Made compact transient command retry when calling compact throws an exception  #75 - Construct export queue row that falls in bucket  #73 - Make compact transient sleep for each range  #70 - Collision Free Map not behaving well when processing backs up  #69 - Compact transient command has negative impact when processing falls behind  #67 - Added option to control number of buckets per tablet  #50 - Renamed Pirto to TableOptimizations",
      "url": " /release/fluo-recipes-1.0.0-incubating/",
      "categories": "release"
    }
    ,
  
    "release-fluo-1-0-0-incubating": {
      "title": "Apache Fluo 1.0.0-incubating",
      "content"	 : "Apache Fluo 1.0.0-incubating is the first release of Fluo as an Apache project and the fourthrelease for the project. Below are resources for this release:      Download a release tarball and verify by these procedures using these KEYS                            fluo-1.0.0-incubating-bin.tar.gz          ASC MD5 SHA1                          fluo-1.0.0-incubating-source-release.tar.gz          ASC MD5 SHA1                      View the documentation for this release.  Read the Javadocs.Starting with 1.0.0-incubating, Apache Fluo will follow semver for all future APIchanges. The API consists of everything under the org.apache.fluo.api package. Code outside of thispackage can change at any time. If your project is using Fluo code that falls outside of the API,then consider initiating a discussion about adding it to the API.Significant changesThe 1.0.0-incubating release includes 167 commits that were made since the last release (1.0.0-beta-2).  The significant changes are summarized below. For information about changesbefore this release, see the archive of Fluo releases before becoming an Apacheproject.Stabilized APIStarting with this release, Apache Fluo’s API will follow semver.  The API is defined aseverything under the org.apache.fluo.api Java package.  Given that the plan is to support this APIfor a long time, a lot of API improvements were made since the beta-2 release.  The following is alist of significant API changes since beta-2.  #772 : Get with default  #770 : Use varargs for Columns  #768 : Made Obsever and Application configuration consistent  #743 : Make data classes final  #723 : Removed FluoAdmin.InitOps()  #714 : Use Charsequence in API  #695 : Removed all 3rd party types from API  #680 : Changed package prefix from io.fluo to org.apache  #647 : Added Oracle and Worker to API  #639 : Improved scanning API  #626 : Added operation to get row+column pairs.  Further improved by #758.  #119 - Consistently offer String in APIConfigurable classpathThe previous releases of Fluo contained specific versions of Accumulo, Hadoop, Zookeeper, and otherFluo dependencies.  However, the actual dependencies needed should be determined by the existingversion of software where Fluo is run.  Fluo changed to a model of making its classpath becompletely configurable by an administrator.  Example classpath configuration and download scriptsship with Fluo to help make setup easier.  These script were only tested with specific versions ofHadoop and Accumulo and may not work with all versions.  These changes were made in #706 and#687.New Fluo version commandAdded a version command to the Fluo scripts in #779.  This makes it easy to automaticallybuild shaded jars with the correct version of Fluo for use in Spark and Map Reduce.User and historical metricsTwo major improvements related to metrics were made.  Support for user metrics was added by#767.  This allows observers to easily report application specific metrics.  In #635the example InfluxDB and Grafana configurations were updated to show historical metrics.Improved transaction processingTransaction processing throughput was increased by changes in #593 and some subsequent bugfixes.   These changes moved commit processing from the threads executing user code to anasynchronous background process.  This allows user threads to queue a transaction for commit animmediately start working on another notification.  This new process allows many more transactionsto be working through the commit process concurrently.  Before the change a worker could only becommitting up to a few hundred transactions at any time.   After the change, a single worker couldeasily be committing tens of thousands of transactions at a time.  These changes increase the latency of individual transactions, which can have some drawbacks #650.   For this release changes weremade in #654 to use a priority queue for notifications in the worker.  The changes in#654 partially solves the lock wait problem identified in #650.Improved Accumulo scanningFluo utilizes server side Accumulo iterators.  For this release these iterators were modified toleverage seeking on the sever side.  Before these changes the iterators used to sequentially scandata.  For popular cells that had a lot versions, this could be slow.  These changes were made in#623.Other changes of interest  #668 - Added method to append byte to BytesBuilder  #648 - Added documentation to help users if YARN is killing containers  #619 #621 - Created Fluo logoTestingA two day run of Webindex was started on September 7th using the latest snapshot version of Fluo.This test used 11 EC2 m3.xlarge nodes.  The test ran without problem.   A good bit before that a 3day run of Webindex was conducted.",
      "url": " /release/fluo-1.0.0-incubating/",
      "categories": "release"
    }
    ,
  
    "blog-2016-06-02-fluo-moving-to-apache": {
      "title": "Fluo is moving to Apache",
      "content"	 : "Fluo was recently proposed as an Apache Podling.  The vote passed andnow we are in the process of moving to Apache.  See INFRA-11900 for moredetails.Currently the only thing setup in Apache land is the mailing list.  If youwould like to subscribe to the new list, send an email todev-subscribe@fluo.incubator.apache.org.  Archives of the list are alsoavailable.",
      "url": " /blog/2016/06/02/fluo-moving-to-apache/",
      "categories": "blog"
    }
    ,
  
    "blog-2016-05-17-webindex-long-run-2": {
      "title": "Running Webindex for 3 days on EC2 Again",
      "content"	 : "Another long run of Webindex was done to test changes made since beta-2and it went well.  This run performed much better than the previous theprevious long run of Webindex.  The experiment was run on 21 EC2m3-xlarge nodes (20 worker nodes).  Each node has 4 cores and 15G of RAM.273.6 million web pages and 3.54 billion links were processed from 8,000 commoncrawl files (each file is around 330M).  It took around 80 hours to load thefiles resulting in a rate of ~950 web page/sec and ~12,292 links/sec.This blog post outlines the changes to Fluo, Fluo Recipes, and Webindex thatmade this long run so much better than the last one.  For anyone writingapplications on Fluo, the changes to Webindex that resulted in improvements maybe of interest.  Unreleased versions of Fluo and Fluo Recipes were used forthis test, so the improvements are not easily available to users yet.  Howeverwe hope to release new versions soon.Rate limited page loadingThe plot below shows how many transactions per second were executed bydifferent observers.  Please refer to the overview in the lastpost for a description of the observers and see the previousplots.  The transactions per second is very even compared to thelast run.   The document load rate was limited to a maximum of 1,000 pages persecond.  There was no limit in the last run, it just ran as fast as it could.The PageLoader is not show in the plot above because of a bug with it in thehistorical view.  In the recent Grafana view it plotted fine and basicallymirrored the PageObserver.The ability to rate limit page loading was added in webindex-70.  For thistest run 20 Spark load task were run, each limited to 50 pages per second.  Thelatest code can sustain much higher rates initially (see thiscomment).  However once a lot of data builds up,compactions and scans in Accumulo start taking more CPU.  A rate of 1,000 pagesper second was chosen as something that could be sustained over multiple days.The CPU plots below show that in the beginning there is idle CPU to spare, butthat does not last.CPU UtilizationIn order to get higher throughput changes were made to reduce CPU usage andevenly spread CPU usage across the cluster.  The following plot shows the CPUusage of all nodes across the cluster during the test.  Unfortunately this datawas not kept for the previous run.  One issue that caused problems in theprevious run was hotpots, where one node was swamped while others were underutilized.  In this run the utilization across the cluster was fairly uniform.The following evenly spread computation :  A short hash was appended to URLs used as the row key for pages.  Thisspread web pages evenly across the cluster.  These changes were made inwebindex-49 and fluo-recipes-45.  The webindex query table schema was changed in webindex-71 to allow largerows to split.  Before this change compactions of large tablets that couldnot split were causing uneven CPU utilization.The following reduced CPU usage :  The two Accumulo tables (Fluo and query table) were configured to use Snappyinstead of GZip.  In fluo-623 the Fluo iterators that run in Accumulo were optimized tosometimes seek.  This resulted in scanning less data in Accumulo to executetransactions and find notifications.  In webindex-54 parsing links was sped up, using less CPU.  Accumulo 1.7.1 was used which has ACCUMULO-4066.  This made processingconditional mutations less CPU intensive.There were 20 worker nodes and 1 master node.  The master node was running thenamenode, zookeeper, resource manager, Accumulo master, Grafana, and InfluxDB.The master node was normally lightly loaded.  However, at one point InfluxDBwas burning lots of CPU for an extended period.  This impacted the namenode,which impacted the entire cluster.  Not sure what InfluxDB was doing, maybesomething like a compaction.  May put it on its own node in future test.The following table shows how the 8,000 files were loaded by 5 spark jobs.After each Spark job completed the two tables were compacted.  Compacting thequery table prevented expensive compactions from occurring during the next load.Compacting the Fluo table cleaned up transaction bookkeeping data.  Thecompactions explain why the CPU utilization is low when the jobs first start.            Num files      Start time      Duration                  2000      5/11 22:32:41      20.8h              2000      5/12 19:34:38      20.7h              2000      5/13 18:19:09      17.7h              1000      5/14 15:09:43      10.5h              1000      5/14 03:10:13      10.5h      Preventing YARN from killing workersIn the previous run Fluo worker processes were constantly being killed by YARNfor exceeding memory limits.  This would cause transactions to have to berolled back.  With the new asynchronous commit changes discussed below a lot ofcommitting transactions could be in flight.  Frequently killing processes withlots of committing transactions would cause lots of rollbacks.This problem was remedied in fluo-593 and zetten-139.  For this run theworkers were given 5.5G with 1.5G reserved.  With these settings no workerswere killed by YARN.  When workers are killed it causes upward spikes in thememory plots.  There are no spikes of individual workers like this in thememory plots below.Twill reserved memory was set by adding the following to yarn-site.xml.fluo-671 was opened to investigate a better way of setting this.  &amp;lt;property&amp;gt;    &amp;lt;name&amp;gt;twill.java.reserved.memory.mb&amp;lt;/name&amp;gt;    &amp;lt;value&amp;gt;1536&amp;lt;/value&amp;gt;  &amp;lt;/property&amp;gt;The following was set in fluo.properties.  io.fluo.worker.max.memory.mb=5632This resulted in workers running with a max heap size of 4096M.  The processeswill grow larger than 4096M, but will not be killed by YARN unless exceeding5632M.Asynchronous commitsIn fluo-593 commit processing was rewritten using an asynchronous model.There is no longer a single thread walking each transaction through the commitsteps.  Instead many transactions are put on a queue for each step andprocessed by a few threads.  This allows many more transactions to beconcurrently committing.   With this model a temporary pause or high CPU loadon a tablet server does not lower throughput.  Before this change, whentservers spiked to 100% this would impact many committing transactions and thethreads running those transactions.  Those threads would wait.  This would leadto lower utilization across the cluster.The plot below shows the number of transactions committing.  As the CPUutilization increase, so does the number of committing transactions.  With thehigh CPU utilization it takes longer for individual transactions to commit, butthroughput is maintained.Unfortunately the plot does not have all data because it was based on recentdata which ages off.  The historical plots for committing transactions is notyet implemented.  See fluo-653.Asynchronous commits offer higher throughput but also increase the commit timeof individual transactions.  As outlined in fluo-650 this can lead toincreased lock wait time when one transaction is waiting on another.  Thisproblem was partially solved by fluo-654 which executes older notificationsfirst.  For Webindex, executing older transactions first works well for thepage data.  However for the Collision Free Maps and Export Queue buckets thatare always being updated, it does not work so well.   A transaction processingthese buckets will usually have lock wait.  The number of buckets was set athalf the total number of worker threads with the thought that this wouldusually leave some threads to process pages.  Not sure if this was helpful.  Abetter solution to fluo-650 is needed.  Below is a plot of lock wait time.Read and Write plotsBelow are plots of the amount data read and written per second by differentObservers.Accumulo SettingsThe following were executed in the Accumulo shell after initializing Fluo butbefore starting the first Spark load job.config -t webindex -s table.compaction.major.ratio=1.75config -t webindex -s table.file.compress.blocksize.index=256Kconfig -t webindex -s table.file.compress.blocksize=64Kconfig -t webindex -s table.file.compress.type=snappyconfig -t webindex_search -s table.file.compress.type=snappyconfig -t webindex_search -s table.split.threshold=512Mconfig -t accumulo.metadata -s table.durability=flushconfig -t accumulo.root -s table.durability=flushconfig -s tserver.wal.replication=2config -s table.file.replication=2Fluo used the webindex table.  A blocksize of 64k was selected to speed uprandom lookups a bit.  A compaction ratio of 1.75 was chosen so that the Fluotable would compact more frequently.  Compactions of the Fluo table run theFluo garbage collection iterator.Webindex uses the webindex_search table for queries and Fluo exports to it.This table uses real data like domain names and URLs, therefore the data doesnot spread evenly.  Lowering the split from the default of 1G to 512M makestablets with popular domains or URLs split and spread across the clustersooner.Data replication was set to 2 because the cluster did not have a lot of spaceand the default of 3 may have filled it up.The Accumulo tserver were configured with a data cache of 3G and an index cacheof 512M.At some point later in the test, the number of compaction threads in Accumulo wasadjusted from 3 to 2.  This was done because there were only 4 cores and havingcompactions use most of them could be disruptive.Fluo SettingsFluo was configured with 128 threads per worker and 20 workers.For the test the following was set in fluo.properties.  This settingsdetermines the maximum amount of transactions that will be held in memory tocommit asynchronously.io.fluo.impl.tx.commit.memory=104857600Webindex settingsWebindex was configured with the following settings.numTablets: 60numBuckets: 1020Each type of data started with 60 tablets.  The types of data are page data,url inlink counts, domain url counts, and the export queue.  1,020 is multipleof 60 giving each tablet the same number of buckets.  With 60 tablets, eachtablet server started off with 3 tablets per data type.   Some of the datatypes split as the test ran.Final Data SizeAfter loading all of the data the two tables were compacted.  The size of thetables is shown below.root@instance16&amp;gt; du webindex         291,525,943,501 [webindex]root@instance16&amp;gt; du webindex_search         271,106,371,976 [webindex_search]Postmortem analysis of the RFiles from this test run lead to work onACCUMULO-1124 and ACCUMULO-4314.Software used  Centos 7  Hadoop 2.6.3  Zookeeper 3.4.8  Accumulo 1.7.1  Fluo 82301a1  Fluo Recipes dd1c373  Webindex 1f9462d  Zetten 43e9cde",
      "url": " /blog/2016/05/17/webindex-long-run-2/",
      "categories": "blog"
    }
    ,
  
    "release-fluo-recipes-1-0-0-beta-2": {
      "title": "Fluo Recipes 1.0.0-beta-2 released",
      "content"	 : "This is the second release of Fluo Recipes which provides common code for Fluo developers.Below are resources for this release:  View the documentation  Read the javadocs: core, accumulo, kryo, spark, test  Jars are available in Maven Central.",
      "url": " /release/fluo-recipes-1.0.0-beta-2/",
      "categories": "release"
    }
    ,
  
    "release-fluo-recipes-1-0-0-beta-1": {
      "title": "Fluo Recipes 1.0.0-beta-1 released",
      "content"	 : "This is the first release of Fluo Recipes which provides common code for Fluo developers.Below are resources for this release:  View the documentation  Read the javadocs:  core, accumulo  Jars are available in Maven Central.",
      "url": " /release/fluo-recipes-1.0.0-beta-1/",
      "categories": "release"
    }
    ,
  
    "release-fluo-1-0-0-beta-2": {
      "title": "Fluo 1.0.0-beta-2 released",
      "content"	 : "Fluo 1.0.0-beta-2 is the third release of Fluo and likely the final release before 1.0.0.Fluo is now at a point where its two cluster test suites, Webindex andStress, are running well for long periods on EC2.Below are resources for this release:  Download the Fluo binary tarball for 1.0.0-beta-2 from GitHub.  View the documentation for help getting started with Fluo.  Javadocs are available for this release.  A tag of Fluo codebase for 1.0.0-beta-2 is available.  The Quickstart and Phrasecount applications were updated to work with this release.This release closed 48 tickets. There is no upgrade path from 1.0.0-beta-1 to1.0.0-beta-2. Many improvements in this release were driven by the creation of two newFluo related projects:      Fluo recipes is a collection of common development patternsdesigned to make Fluo application development easier.  Creating Fluo recipesrequired new Fluo functionality and updates to the Fluo API.  The first releaseof Fluo recipes has been made and is available in Maven Central.        WebIndex is an example Fluo application that indexes links to webpages in multiple ways.  Webindex enabled the testing of Fluo on real data atscale.  It also inspired improvements to Fluo to allow it to work better withApache Spark.  Significant featuresThis release contains many new features that makes it easier to run, develop, and monitor Fluo applications.Improved Fluo metrics that can be sent to InfluxDB and viewed in GrafanaIn #569, #570, &amp;amp; #580, Fluo metrics and monitoring were refactored to fix severalbugs and allow metrics to be sent to InfluxDB and viewed in Grafana.  Fluo metrics are still instrumented using Dropwizard metrics but in #574 metrics configuration was moved from its own file and to now reside in fluo.properties.  While Fluo metrics can still be sent to many different tools (i.e Graphite, Ganglia), Fluo now ships with configuration that can be used to configure a Fluo dashboard in Grafana that queries InfluxDB.  To set up the sending of Fluo metrics to InfluxDB/Grafana,view this documentation or consider using Fluo-dev or Zettento run Fluo as they can install InfluxDB+Grafana and setup metrics for you.Improved Fluo administrationSeveral commands were added to the fluo script which is used to administer Fluo.  A fluo exec command(#581) was created to provide an easy way to execute application code using Fluo classes and dependencies.A fluo list command (#523) was created to let users list all Fluo applications within a Fluo instance.The fluo scan command now has a --raw option (#597) that prints Fluo data as stored in Accumulo. Thiswas accomplished by moving the Fluo formatter from Accumulo shell to the scan command.  The scan command can nowformat non-ascii characters as hex (#568).  The fluo new command was improved to prevent users fromsetting invalid Fluo application names (#510).  A bug was fixed in the fluo start command that was causingtime outs when starting applications (#562).  Finally, the fluo command can now be run without the apps/directory configured for an application for most commands (#524). Only the init and start commands needthis directory configured.  All other commands only require the default properties file to be configured at conf/fluo.properties.Made Fluo work better with SparkSeveral changes were made to Fluo to allow it work better with Apache Spark.  All Fluo data types now implementSerializable and can be use in Spark RDDs (#539).  Fluo data types also now implement Comparable whichallows RDDs of Fluo data types to be sorted (#544).  Also, a no args constructor was created for theMutableBytes data type to enable Kryo serialization in Spark (#549).  Finally, a new InputFormat calledFluoEntryInputFormat was created that returns key/value entries and the existing FluoInputFormat that returnsrows was renamed FluoRowInputFormat (#538,#540).Performance improvementsA good bit of time was spent analyzing Fluo while it was running to determinewhere time is spent when executing transactions.   Based on this analysis, itwas found that a good bit of time was spent committing transactions.  Changeswere made in Fluo and Accumulo in order to decrease commit time.  For Fluo, thefollowing changes were made :  #591 - Shared batch writer increases transaction history  #590 - Increased batch writer threads and made configurable  #589 - Added 2nd conditional writer and logging of commit times  #584 - Adjust number of conditional writer threads based on cluster sizeFor Accumulo, changes are being made in ACCUMULO-4066 to decrease the time ittakes to process conditional mutations.  Conditional mutations are used whenFluo commits a transaction.These changes resulted in nice improvements over beta-1 in testing.  However thereis probably still room for improvement.  More analysis is needed.API ChangesOnce Fluo 1.0.0 is released, all releases after that will followsemver.  For now some small API changes are still beingmade.  The following API changes happened between beta-1 and beta-2.  #566 - Added RowColumnValue and made Accumulo init code use it  #551 - Added method to get start timestamp of transaction  #550 - Changed setObservers() to addObservers()Other important improvements and bug fixes  #598 - Upgraded Hadoop to 2.6.3 and Accumulo to 1.6.4  #587 - Specified datasource for all graphs in fluo’s Grafana dashboard  #586 - Added efficient and easy way to build Bytes objects  #578 - Plot nothing in Grafana when no data exists  #573 - Fixed issues building against Accumulo 1.8.0-SNAPSHOT  #561 - Stopped checkstyle mvn plugin from running at validate  #559 - Eventually drop deleted data  #558 - Added arguments to deploy command to skip findbugs, checkstyle, and auto-formatting  #556 - Make TravisCI deploy snapshot jars after successful builds  #552 - Made eclipse stop complaining about unknown plugins  #547 - Provide better documentation for LoaderExecutor  #535 - Upgraded Twill to 0.6.0-incubating  #520 - Consolidate all implementation properties into FluoConfigurationImpl  #518 - Make Oracle run on a random port  #513 - Unable to pass spaces to scan command  #495 - Add support for notifications to Fluo formatterTestingFor this release, a long run of the Webindex application was performed and is documented ina blog post.  A long run of Fluo stress was run and documented in another blog post.",
      "url": " /release/fluo-1.0.0-beta-2/",
      "categories": "release"
    }
    ,
  
    "blog-2016-01-11-webindex-long-run": {
      "title": "Running Webindex for 3 days on EC2",
      "content"	 : "In preparation for the Fluo beta 2 release, Webindex, an exampleFluo application, was run on 24 EC2 m3-xlarge nodes (20 worker node) for 3days.  The experiment went well as Webindex indexed 178 million web pages and2.32 billion links. The average rate for the entire test was 698 pages/sec and8,943 links/sec.  A few problems with Fluo Recipes and Webindex werediscovered during the course of the test.Webindex OverviewWebindex indexes URLs from Common Crawl data in multiple ways into anexternal Accumulo table. Webindex offers a simple web applicationfor querying this external Accumulo table.  The following three questions canbe answered from this web page.  Which page is the most linked to?  Which page in a domain is the most linked to and how many pages were seen for a domain?  For a given page, what links to it and how many links are there?In order to answer these questions, Webindex continually updates the followingthree indexes in an external Accumulo table.  A per domain index containing linked to counts in descending order.  A total index containing linked to counts for all pages in descending order.  A per page index containing the pages incoming and outgoing links and incoming link count.The following is a simple example of some of what Webindex does.  In order tokeep it simple, the example below does not show everything Webindex does.  Content for http://A.com that links to http://B.org is loaded.  Content for http://C.com that links to http://B.org is loaded.  The inbound link count for http://B.org is computed as 2.  (2,http://B.org) is inserted into the external domain index, total index, and page count.  Content for http://C.com that links to http://B.org is loaded.  The inbound link count for http://B.org is computed as 3.  (2,http://B.org) is deleted from all the external indexes and (3, http://B.org) is inserted.Videos from runDuring the course of the 3 day run, screenshots of different queries in theWebindex web application were taken every 6 minutes.  From these screenshots,the following videos were created showing Fluo in action. Keep in mind that asthe counts are changing, the old count is deleted and a new count is inserted.If Fluo ever dropped the ball on this, it would leave both entries in theexternal index causing the same URL to show up twice with different counts.The video below shows querying for the most linked to page every 6 minutes.The video below shows querying for which pages at stackoverflow.com are the mostlinked to.  The count at the top is the total number of pages seen in thewikipedia.org domain.Videos were also created for apache,wikipedia,github, and cnnSome custom scripts and pageres-cli were used to generate the screenshots.Videos with a screenshot every minute are also available.  If interested inseeing these, contact the Fluo dev list.Grafana plotsThe Grafana plots below are from the three day run. No plot of notificationsqueued is shown because processing of pages kept up and never fell behind. Tounderstand the plots, below is a simple description of what the differentobservers do.  Observers are written by Fluo users, run by Fluo workers,  andrespond to notifications and execute transactions.  The PageLoader runs in a Spark job and loads pages parsed from a CommonCrawldata file.  The PageObserver processes updates to pages, determines what links changed,and pushes link updates to other observers.  The CollisionFreeMapObserver computes linked to counts and pushes the countsto the ExportObserver.  This observer also rolls up domain counts and pushesthose for export (calling different code and operating on a different part ofthe table).  The ExportObserver updates the external Accumulo index table in a faulttolerant manner.Problems FoundThe test was useful in that it showed Fluo is probably ready for anotherrelease. While no show stoppers were found, it was also useful in that manyareas where improvements could be made were found.During the first day of the test, it was noticed that major compactions inAccumulo were sucking up a lot of resources.  Each node only had four cores.Before the test were started, compaction ratios were adjusted.   Fluo’s tablewas set to 1.5 and the external index table was set to 2.  These ratios coupledwith the fact that the compactions were using gzip caused really high load,which eventually caused processing to drop to around 500 pages/sec.  While thetest was running the compaction ratios were adjusted to 2 and 3, and compactionwas set use snappy.  This caused performance to increase.  A few more coreswould have been nice.The Spark load jobs have to parse, validate, and clean up all of the links.This seems to be very CPU intensive and may be the current bottleneck (seefluo-io/webindex#41). It seems that as Accumulo’s CPU loadincreased (because of compactions) that the load rate dropped.  As anexperiment during the 2nd day of the test, loaders were stopped and two loadersper node were started.  This resulted in a higher load rate of around 1000pages/sec.  In order to ensure the test would run over night, this was onlydone for a brief period before reverting to one loader per node.  Again, morecores would have been nice.  In place of more cores, looking into optimizing thisand getting a higher load rate would nice.  Another issue noticed with loadingwas fluo-io/webindex#39.While monitoring the test it became apparent that split points for the Fluotable were not equally spreading computation across all tablet servers. Seefluo-io/fluo-recipes#44 for more information.  Somewhere around the2nd day of the test, tablets were merged and new splits points were added whilethe test was running.  This worked really well.  Another issue found relatingto split points was fluo-io/webindex#40.The lack of metrics about Conditional mutations from Accumulo tablet serversmakes it hard to track down problems where many Fluo transactions are hitting afew tablet servers.  The bad split points mentioned in the previous paragraphare one example of this type of problem.  ACCUMULO-4107 wascreated to address this issue.Some skew was noticed as a result of YARN’s placement decisions.  This wascaused by coordination task displacing CPU intensive task.  For exampleFluo has three task types that run in YARN : Oracle, Twill Application manager,and Workers.  Ideally there would be one Worker per node. However, because ofthe Oracle and Application task, YARN may place two workers on a single node.Opened fluo-io/fluo#600.During the test, worker task were dying or being killed because of memoryoveruse.  Twill/YARN automatically restarted the workers and the test keptrunning w/o much problem.  It would be good to avoid this since this causestransaction recovery or rollback.  See fluo-io/webindex#42.The test identified a possible need for HashedRow recipe to moreevenly distribute processing of page related transactions.Test environmentThe following configuration and software were used for this test.  24 m3.xlarge EC2 nodes. 15G ram, 4 cores, and 2x35G SSD.  Centos 7  Hadoop 2.6.0  Zookeeper 3.4.7  Accumulo 1.8.0-SNAPSHOT with ACCUMULO-4066 patches  Fluo beta-2-SNAPSHOT 78bcdb7  Fluo recipes beta-1-SNAPSHOT 96858d0  Fluo deploy 43bf08f  2G data cache and 768M index cache in Accumulo  64 threads in Accumulo client pool and readahead pool  128 worker threads and 4G per worker (initially went to 5G and 6G as test was running).  20 Load task running in SparkStorage statisticsAfter the three day run completed, the external index table had 4.71 billionentries and used 191G in HDFS.   A compaction was forced on the Accumulo table,which caused old versions and deleted data to be dropped.  After the compactionthe table had 4.02 billion entries and used 159G in HDFS.  The table was beingfrequently updated by Fluo and that’s why so much data was dropped by thecompaction.  The way Accumulo works, compactions were continually happeningwhile test was running.  So the total amount of data dropped from the table asa result of updates is unknown.The table used by Fluo had 1.46 billion entries at the conclusion of the testand used 93G in HDFS.Future WorkMost Fluo scale testing to date has been on EC2.  It would be really nice totest Fluo on bare metal.  We are going to experiment with getting FluoDeploy to work on bare metal whereCentOS 7 is already installed.Seeing Application level stats plotted in Grafana, as outlined influo-io/fluo#534, would be really nice.  For webindex this wouldinclude things like URLs exported per second, domains exported per second, etc.This issue was not identified during this test, it just would have been reallynice to have this information while running the test.Eventually this test needs to be run on 10, 20, and 40 nodes to measure therelative performance increase as the number of nodes is increased.  Ideallythis test would run on bare metal.",
      "url": " /blog/2016/01/11/webindex-long-run/",
      "categories": "blog"
    }
    ,
  
    "blog-2015-12-22-beta-2-pre-release-stress-test": {
      "title": "Beta 2 pre-release stress test",
      "content"	 : "In preparation for a beta 2 release, the stress test was run again on EC2.The test went well outperforming the first stress test and beta-1 stresstest.For this test run, initially ~1 billion random integers were generated andloaded into Fluo via map reduce.  After that, 1 million random integers wererepeatedly loaded 20 times, sleeping 10 minutes between loads.  Aftereverything finished, the test was a success. The number of unique integerscomputed independently by MapReduce matched the number computed by Fluo.  Bothcomputed 1,019,481,332 unique integers.The test took a total of 7 hours 30 minutes and 30 seconds.  Over this timeperiod 61.7 million NodeObserver and 20 million NodeLoader transactions wereexecuted.  The average rate of transactions per second for the entire test was2,968 transactions per second.  At the conclusion of the test, the stress tablehad 3.87 billion entries.The test was run with the following environment.  18 m3.xlarge worker nodes  18 Fluo workers, each having had 4G memory and 128 threads  18 Map reduce load task, each with 32 threads  18 Tablet servers, each with 3G (1.5G for data cache, .5G for index cache, and .5G for in memory map)  Fluo built from c4789b3  Fluo stress built from 32edaf9  Accumulo 1.8.0-SNAPSHOT with ACCUMULO-4066 patch.Grafana plotsAn exciting new development in the Fluo eco-system for beta-2 is theutilization of Grafana and InfluxDB to plot metrics.  Also metricsconfiguration was simplified making it possible to report metrics from MapReduce and Spark. In the plots below we can see metrics from the loadtransactions executing in Map Reduce.  In previous test, this was not visible,being able to see it now is really useful.Notifications were building up during the test. A better method than sleepingbetween loads, as mentioned in fluo-io/fluo-stress#30, is still needed.Short runsBefore starting the long run, a few short runs loading 1 million few times weredone with an empty table.Further testingA long run of webindex will also be run on EC2 before releasing beta-2.",
      "url": " /blog/2015/12/22/beta-2-pre-release-stress-test/",
      "categories": "blog"
    }
    ,
  
    "release-fluo-1-0-0-beta-1": {
      "title": "Fluo 1.0.0-beta-1 released",
      "content"	 : "Fluo 1.0.0-beta-1 is the second official release of Fluo. Fluo is an implementation of Google’s percolator paper, which adds large-scale incremental processing of data using distributed transactions and notifications. It runs on YARN and stores its data in Accumulo.Below are resources for this release:  Download the Fluo binary tarball for 1.0.0-beta-1 from GitHub.  View the documentation for help getting started with Fluo.  Javadocs are available for this release.  A tag of Fluo codebase for 1.0.0-beta-1 is available.  Fluo jars have been deployed to Maven Central.  The Quickstart and Phrasecount applications were updated to work with this release.This release closed 133 tickets. This release is not recommended for production use.There is no upgrade path from 1.0.0-alpha-1 to 1.0.0-beta-1.Significant featuresThis release contains many new features that makes it easier to run, develop, and monitor Fluo applications.Simplified Fluo administration on a local machine or EC2 clusterDevelopers can now run Fluo and its dependencies on their local machine (#92) or an AWS EC2 cluster (#356) using a few simple commands.This was done by creating two administration tools called Fluo-dev and Fluo-deploy whose scripts and configuration reside in reposseparate from the Fluo code base.  These tools allow developers to collaborate and share configuration for running Fluo.Transaction metrics are viewable using common monitoring toolsFluo now publishes metrics (#20) about transactions, collisions, and timestamps using Dropwizard metrics.  These metrics are by default published using JMX and are viewable using JConsole or JVisualVM.  Fluo can also be configured to publish metrics to Graphite or Ganglia.  View the metrics documentationfor more information.Improved processing of notifications in Fluo workersFluo workers were refactored to separate the code used for finding and executing work.  Each worker uses a single thread for finding work (#19).  A new method was introduced to partition work among workers using a hash+mod of notifications (#282).The commit message for 4100e23 contains a good description of some of the benefits and drawback of the currenthashing approach.Improved the deletion of observer notificationsWhen a cell is deleted in Accumulo, a delete marker is inserted.  Delete markers stay around untilall files in a tablet are compacted.  For Fluo this could cause a lot of notification delete markersto build up over time.  To avoid this buildup, the way Fluo deletes notifications was changed in(#457).  Accumulo delete markers are no longer used, Fluo now uses a custom delete marker fornotifications.  The custom deleter marker allows Fluo to do analysis when Accumulo flushes memory todisk and avoid writing many delete markers to persistent storage.Easier management of Fluo from the command lineFluo now provides different scripts (fluo, mini-fluo, &amp;amp; local-fluo) for managing Fluo using a YARN cluster, MiniFluo, or local processes.  Several commandswere created for these scripts.  A scan command allows users to print a snapshot of a Fluo table (#319).  A info command shows locations of containerswhen running Fluo in YARN (#297).  A classpath command gives users a list of jars needed to execute Fluo client code (#436).  A wait command willsleep until all notifications are processed (#434).Support for running multiple Fluo applications on a single clusterUsers can now run multiple Fluo applications using a single cluster (#454).  This enables different Fluo users to share the same cluster.  Fluo applicationscan be started and stopped independently.  Each application has its own configuration.Fluo build improvementsOn each build, all Java code is automatically formatted based on Google Java Style (#479).  Also, checkstyle and findbugs will fail the build if certainstandards are not reached (#185).  The POM is also sorted (#493).Organized Fluo code baseThe Fluo stress test was moved to its own repo and is no longer a sub-module (#385).  MiniFluo was moved from fluo-core to the fluo-mini module/jar (#439).  This reduced the number of dependencies in fluo-core.  However, developers will now need to include the fluo-mini jar in their MavenPOM if they start MiniFluo.Fluo testing improvementsIntegration tests can now be run from Eclipse (#322).  Several new unit tests were created.Other important improvements and bug fixes  #470 - Replaced FluoFileOutputFormat with an Accumulo Key/Value generator  #460 - Reduced Fluo API module dependencies  #456 - Fixed bug with notifications being lost when processes died  #446 - Simplified log configuration and configure rolling log files in YARN  #442 - Reduced the number of curator clients in FluoAdmin  #383 - Improved transaction logging to help users debug collisions. See debugging documentation.  #365 - Analyze Fluo code to see what non-public Accumulo APIs are used  #362 - Made API data objects immutable  #349 - Support application level configuration in fluo.properties  #342 - Add a configurable retry timeout to Fluo clients  #294 - Fluo now uses chroot suffix in its Zookeeper connection.  #293 - Add argument checking to FluoConfiguration  #244 - Make re-initialization easier for userTestingA successful long stress test run was conducted using Fluo built from commitfb647dd.  The test ran very well and never fell behind like aprevious long run of stress did.  The test had the followingproperties.  Initialized stress test using 1 billion random integers.  Ran 150 incremental loads of 100 thousand integers.  Slept 3 minutes between loads.  Used 19 m3.xlarge nodes on EC2.  16 workers and 3 masters  Configuration for the test committed and tagged in git : fluo-deploy tag and fluo-stress tag  Opened two issues as a result of test #499 and fluo-stress#30Below is the trailing output from running the test.*****Generating and loading incremental data set 148**********Generating and loading incremental data set 149**********Generating and loading incremental data set 150**********Calculating # of unique integers using MapReduce*****                UNIQUE=1014486419*****Wait for Fluo to finish processing*****05:33:40.158 [main] INFO  io.fluo.cluster.runner.AppRunner - The wait command will exit when all notifications are processed05:33:40.417 [Thread-3] INFO  io.fluo.core.oracle.OracleClient - Connected to oracle at worker4:991305:33:41.308 [main] INFO  io.fluo.cluster.runner.AppRunner - All processing has finished!*****Printing # of unique integers calculated by Fluo*****Total at root : 1014486419Nodes Scanned : 59605*****Verifying Fluo &amp;amp; MapReduce results match*****Success! Fluo &amp;amp; MapReduce both calculated 1014486419 unique integersThe test ran for a little over 12 hours.  Below are two plots pulled fromgraphite showing the number of notifications queued and transaction rate overthe entire test run.  Load transactions are not included in the rate.  The rateis transactions per second.  The accuracy of these plots is uncertain becauseno graphite configuration changes were made.  The plots do seem within theballpark. ",
      "url": " /release/fluo-1.0.0-beta-1/",
      "categories": "release"
    }
    ,
  
    "blog-2015-05-22-fluo-talk-at-accumulo-summit": {
      "title": "Fluo talk at Accumulo Summit",
      "content"	 : "A talk on Fluo was given at the Accumulo Summit on April 29, 2015.  Below are the slides from the talk: ",
      "url": " /blog/2015/05/22/fluo-talk-at-accumulo-summit/",
      "categories": "blog"
    }
    ,
  
    "blog-2014-12-30-stress-test-long-run": {
      "title": "First long stress test run on Fluo",
      "content"	 : "Fluo has a stress test which computes the number of unique integersthrough the process of building a bitwise trie.  Multiple collections ofrandomly generated integers are provided as input.  The test suite includes asimple map reduce job that can read the same data fed to Fluo and compute thenumber of unique integers. The correctness of Fluo can be verified by checkingthe result in Fluo against the map reduce job.  The test is intended toexercise Fluo at scale and complements the unit and integration test.  One ofour goals before releasing beta is to run this test on clusters for longperiods of time.  This post records the experience of running the stress testovernight on a cluster for the first time.For this test run, initially ~1 billion random integers were generated andloaded into Fluo via map reduce.  After that ~100K random integers wererepeatedly loaded 120 times, sleeping 2 minutes between loads.  Aftereverything finished, the test was a success. The number of unique integerscomputed independently by MapReduce matched the number computed by Fluo. Belowis the output of the stress test count from Fluo. $ java -cp $STRESS_JAR io.fluo.stress.trie.Print $FLUO_PROPS Total at root : 1011489250 Nodes Scanned : 59605Below are a few lines of output selected from the map reduce job. Map input records=1011999273 io.fluo.stress.trie.Unique$Stats     UNIQUE=1011489250This output shows that 1,011,999,273 random integers (between 0 and1012-1) were given to Fluo and map reduce.  Both computationsreported 1,011,489,250 unique integers.Graphite PlotsBefore running the overnight test, a quick test with only a few iterations wasrun against an empty table.  This initial test went well and had no problemskeeping up.  Based on that quick test, the decision was made load 100K randomintegers into Fluo every two minutes for 120 iterations.  However a bigdifference between the quick test and the long running test, was that the longrunning test did not start with an empty table. What worked well for an emptytable did not work well for a table with a billion initial entries.  The longrunning test was kicked off in the evening, giving EC2 something to do in thewee hours.In the morning the long running test was still running, but had fallen behind.The image below shows transaction committed per minute and covers a 15 hourperiod.  Unfortunately this image does not include the load transactions, therewas a problem getting that reported to Graphite.  So only Observer transactionsare shown in the plot.The image below shows notifications queued and covers a 15 hour period.Upon seeing that the test was falling behind, fiddling with it was unavoidable.The table was configured to use bloom filters and a compaction forced, with thehope that this would lead to less file accesses.  This caused performance todrop for some reason.  Since that did not work, different bloom filter settingwere set and another round of compactions forced.  The new settings causedtablet servers to start dying with out of memory errors.  The Fluo workerscontinued to limp along using a subset of tablet servers.   The fiddling andfutzing was declared a failure, bloom filter settings reverted, another roundof compactions issued, and tservers restarted.  Around the time the fiddlingended, so did the map reduce jobs that were loading new data.  It was verysatisfying that the counts came out correct after all of this disruptiveactivity.The plots below cover a 21 hour period and overlap in the 1st 15 hours with theplots above.  The drops in performance are due to the previously mentionedshenanigans.  The dramatic recovery is due to load transactions finishing andcompacting away bloom filters.The plot below shows transactions committed per minute.The image below shows notifications queued.Something mysterious in the plot above is the difference between min and maxqueue sizes.  To investigate, Graphite was used to plot each workers queue sizeover time.  This plot is shown below.  Its hard to see from the plot, but 10workers start falling behind almost immediately and 7 did not.  No idea whathappened here.Test EnvironmentThe test was run on 20 m1.large EC2 nodes using the script at the end of thisREADME.  The parameters in the script were set as follows, the for loopwas changed to {1..120}, and the sleep time was set to 120.MAX=$((10**12))SPLITS=68MAPS=17REDUCES=17GEN_INIT=$((10**9))GEN_INCR=$((10**5))The following was used to run the test :  Custom balancer  accumulo-site.xml  fluo.properties  Table settings  Fluo from commit acf1ea7d8d6bc74eef7e311008e5e8fc0fd94d30  Accumulo 1.6.1  Hadoop 2.6.0  Centos 6.5ConclusionWe are very happy the counts came out correct especially since some tabletservers died (which was unplanned).  In a later test, we can hopefully killtablet servers, Fluo workers, and datanodes.Looking at the numbers leads to the question: was the performance good?  Atthis point that is unclear.   Need to get a sense of what the theoreticalmaximum rate would be based on basic performance characteristics of nodes.That mathematical model does not exist at the moment.The particular EC2 nodes used in this experiment are not very speedy.  A singlehigh end workstation can match the performance of these 20 nodes, howeverscaling issue will never be seen on a single node.  The m1.large nodes wereused because they were cheap.  Many scaling issue were found using these nodes.After working out bugs on the cheaper nodes, we may run experiments using moreexpensive, high performance EC2 nodes.  However this will depend on #356which should make that process easier.  The current 20 node m1.large setup waspartially manually setup.Further experiments could be done adjusting various Accumulo and Fluo settings,like number of threads.  Also, it will be interesting to see what impactimplementing issues like #12 will have.",
      "url": " /blog/2014/12/30/stress-test-long-run/",
      "categories": "blog"
    }
    ,
  
    "release-fluo-1-0-0-alpha-1": {
      "title": "Fluo 1.0.0-alpha-1 released",
      "content"	 : "Fluo 1.0.0-alpha-1 is the first official release of Fluo. It contains 83 initial tickets marked for this first milestone. The roadmap for this release included a working initial implementation, completing an initial pass at the client API (which will be finalized in 1.0.0), and writing a stress test application.  This release has not been tested much at scale and is not ready for production use.  More scale and performance testing will be done for the beta release.Below are resources for this release:  Download the Fluo binary tarball for 1.0.0-alpha-1 from GitHub.  View the documentation for this release.  Read the Javadocs.  The Fluo jars have been deploy to Maven Central.Fluo is an implementation of Google’s percolator paper, which adds large-scale incremental processing of data using distributed transactions and notifications. It runs on YARN using Twill and stores its data in Accumulo.Significant featuresThis alpha release contained a lot of initial features to help establish the basic implementation and client API. All of these features will be expected to become more robust over time.MiniFluoMiniAccumulo and MiniFluo make it easy to start a local Fluo instance for integration testing. MiniFluo also makes it easy for developers to experiment with Fluo locally without installing it on an Accumulo cluster.Garbage CollectionWithout garbage collection, the columns in a Fluo table would grow without bound as updates are applied over time. Because of this, it’s important to clean up old fully-processed (and no longer being read) transaction data. The initial implementation of garbage collection kept N versions of columns. Fluo-8 added the ability to garbage collect all column versions that are no longer being read by transactions anywhere else in the cluster.MapReduce InputFormatThe FluoInputFormat allows a specific snapshot to be read into a mapreduce job. It was established in Fluo-7.FluoFileOutputFormat and FluoOutputFormatThe FluoFileOutputFormat enables the bulk ingest of a Fluo table using mapreduce by creating the Accumulo r-files in HDFS. The FluoOutputFormat pushes keys directly into the Accumulo tablet servers through the client API. Fluo-35 added this feature.Fluo Workers and Oracle running in YarnFluo can use Yarn to manage fault tolerance and guarantee multiple worker processes and some number of Oracle processes are kept running across the cluster. Apache Twill was used for the Yarn integration. Fluo-11 added an application that will deploy Fluo to a Yarn cluster.Fluo APIFluo provides a fluent-builder API for creating transactions to read and mutate data of various different data types in the Fluo table. This API was finalized in Fluo-23 and Fluo-98 when it was moved into its own module.  It includes a type layer (finished in Fluo-153) that makes it easier to work with different types, default values, null return types, and row/column maps.Stress testA stress test was created that uses a tree to force collisions on transactions trying to mutate multiple columns at the same time. This was established in Fluo-24.Other important improvements and features:  Fluo-1     - Create scripts for starting/stopping services  Fluo-3     - use Curator for leader election and leader discovery  Fluo-13   - Register clients that perform transactions in Zookeeper  Fluo-15   - Leader election for the Oracle Server  Fluo-16   - Allow observer notification to be configured in AbstractObserver class  Fluo-89   - OracleApp should allow configurable number of instances  Fluo-98   - Create a Range class for Fluo  Fluo-123 - Hide implementation of Bytes in the API  Fluo-130 - Expose MiniFluo in public API  Fluo-131 - Make LoaderExecutor implement closeable  Fluo-147 - Create fluo-accumulo module for Accumulo iterators and formatter  Fluo-175 - Add newTransaction() method to FluoClient  Fluo-186 - Add Exceptions to APITestingAll unit and integration tests passed.  Two stress tests were run successfully on a 20-node cluster with one test generating 2000 integers and another generating 5000 integers.  The 5000 integer test took about 10 minutes to run after the MapReduce load of integers was complete.Current Status of Percolator ImplementationThe following tables identify the features necessary for a full implementation of Percolator and the current state of Fluo with regards to the implementation. For alpha, a test to exercise some of these features at scale was created, however, it was not run at scale for long periods. One of the goals for beta is to have at least a 24hr run of the scale test, on at least 20 nodes, succeed.Necessary features:            Percolator Feature      Details      Testing                  ACID-compliant cross-row transactions      Provides distributed transactions with ACID guarantees that work across Accumulo rows. Rows are distributed across tablet servers.      IT,CST,#2401              Observers      Perform transactions by sending read/write RPCs to Accumulo tablet servers, which in turn send read/write RPCs to HDFS.      IT,CST              Worker Process      Scans Accumulo for changed columns and invokes necessary observers as function calls on the worker.      IT,CST              Timestamp Oracle      Provides strictly increasing timestamps; a property required for correct operation of the snapshot isolation protocol.      IT,CST              Oracle Failover      When an active Oracle process dies, timestamps should be served from a different Oracle. They should never go backwards.      IT,#37,#2411              Lightweight Lock Service      Makes searches for dirty notifications more efficient. Zookeeper is used in this case. Should be low-latency, replicated, balanced, and persist to a backing store.      IT,CST              Multithreaded Transactions      Allows highly parallel execution of transactions and synchronization only during get() and commit() upon which transactions actually collide.      IT,CST              Snapshot Isolation      Non-serializable isolation mostly implemented through MVCC provided through Accumulo’s timestamp portion of the key.      IT,CST,#2401              Roll-back / Roll-forward semantics      Failures during the first phase of commit cause transactions to be rolled back while failures during the second phase can cause them to be rolled forward.      IT,#2401      1 This feature has been at least partially tested.Performance enhancements:            Enhancement      Details      Testing                  Locality Group for NOTIFY column      Add an extra index in the underlying r-files for the notify column where it can be scanned faster by workers.      IT,CST              Worker scan conflict resolution      Workers register random tablets they choose to scan. When two workers generate the same tablet, one of them re-generates a new random tablet.      #5              Worker advertisement      Workers advertise through the lock service so that the system can know if they failed and any transactions upon which they are working can be rolled back or rolled forward      IT              Oracle batching      Timestamps get allocated in batch to workers. Each worker should share one connection to Oracle to limit RPCs across threads      IT,CST              Weak Notifications      Allows notification of a cell without the possibility of transactional conflict through a special type of column that can be notified but not written to      IT              Conditional Mutations      Allows READ-MODIFY-WRITE of a mutation in a single RPC.      IT,CST              Conditional Mutation batching      Delay sending of conditional mutations destined for same tablet server so they can be batched. Lowers number of RPCs, adds latency but provides better parallelism.      IT,CST              Read batching      Delaying of read operations on tablet server for batching      #138              Prefetching &amp;amp; read-ahead caching      Prediction made each time a column is read, which other columns will be read in the same transaction based on previous behavior.      #12        IT = Integration tests, CST = Cluster stress tests  Testing columns providing a ticket number have not yet been implemented.QuickstartIf you are new to Fluo, the best way to get started is to follow the quickstart which starts a local Fluo instance (called MiniFluo).  By using MiniFluo, you can avoid following the steps below to run your own Fluo instance.Running a Fluo instanceIn order to run a Fluo instance, you will need Accumulo (version 1.6+), Hadoop (version 2.2+), and Zookeeper installed andrunning on your local machine or cluster.Fluo distributions are built for specific releases of Hadoop and Accumulo.  If you are using Accumulo 1.6.1 and Hadoop 2.3.0, youcan download the 1.0.0-alpha-1 tarball. Otherwise, you will need to clone the Fluo repo and build a distribution tarballof 1.0.0-alpha-1 for your environment:git clone https://github.com/fluo-io/fluo.gitcd fluo/git checkout 1.0.0-alpha-1mvn package -Daccumulo.version=1.6.1-SNAPSHOT -Dhadoop.version=2.4.0The distribution tarball will be created in modules/distribution/targetWhen you have a Fluo distribution, install it to a directory:tar xzf fluo-distribution-1.0.0-alpha-1-bin.tar.gzNext, follow the instructions to configure and run Fluo in the README.Running Fluo applicationsOnce you have Fluo installed and running on your cluster, you can now run Fluo applications.  Fluo applications consist of clients and observers. If you are new to Fluo, consider first building and running the phrasecount application on your Fluoinstance.  Otherwise, you will need to include the following in your Maven pom and follow the steps below to create your ownFluo client or observer:&amp;lt;dependency&amp;gt;  &amp;lt;groupId&amp;gt;io.fluo&amp;lt;/groupId&amp;gt;  &amp;lt;artifactId&amp;gt;fluo-api&amp;lt;/artifactId&amp;gt;  &amp;lt;version&amp;gt;1.0.0-alpha-1&amp;lt;/version&amp;gt;&amp;lt;/dependency&amp;gt;&amp;lt;dependency&amp;gt;  &amp;lt;groupId&amp;gt;io.fluo&amp;lt;/groupId&amp;gt;  &amp;lt;artifactId&amp;gt;fluo-core&amp;lt;/artifactId&amp;gt;  &amp;lt;version&amp;gt;1.0.0-alpha-1&amp;lt;/version&amp;gt;  &amp;lt;scope&amp;gt;runtime&amp;lt;/scope&amp;gt;&amp;lt;/dependency&amp;gt;Creating a Fluo clientTo create a FluoClient, you will need all properties with the io.fluo.client.* prefix set in the configuration that is passedto the newClient() method of FluoFactory.  See fluo.properties for details on these properties.Creating a Fluo observerTo create an observer, follow these steps:  Create a class that extends AbstractObserver.  Build a jar containing this class and include this jar in lib/observers of your Fluo installation.  Configure your Fluo instance to use this observer by modifying the Observer section of fluo.properties.  Restart your Fluo instance so that your Fluo workers load the new observer.",
      "url": " /release/fluo-1.0.0-alpha-1/",
      "categories": "release"
    }
    
  
}
